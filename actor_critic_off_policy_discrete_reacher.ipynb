{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-21 19:09:32,635] Making new env: Reacher-v1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import mujoco_py\n",
    "\n",
    "np.random.seed(2)\n",
    "tf.set_random_seed(2)  # reproducible\n",
    "\n",
    "# Superparameters\n",
    "OUTPUT_GRAPH = False\n",
    "MAX_EPISODE = 300000\n",
    "DISPLAY_REWARD_THRESHOLD = -10  # renders environment if total episode reward is greater then this threshold\n",
    "MAX_EP_STEPS = 100 # maximum time step in one episode\n",
    "RENDER = False # rendering wastes time\n",
    "GAMMA = 0.99     # reward discount in TD error\n",
    "ALPHA = 2\n",
    "LR_A = 0.00001    # learning rate for actor\n",
    "LR_C = 0.00001    # learning rate for critic\n",
    "# env = gym.make('MountainCarContinuous-v0')\n",
    "env = gym.make('Reacher-v1').env\n",
    "env.seed(1)  # reproducible\n",
    "\n",
    "N_F = env.observation_space.shape[0]\n",
    "# Reacher\n",
    "action_res = [3, 3]\n",
    "N_A = action_res[0]*action_res[1]\n",
    "action_map = np.zeros([np.prod(action_res), 2])\n",
    "u = np.linspace(env.action_space.low[0], env.action_space.high[0], num=action_res[0])\n",
    "v = np.linspace(env.action_space.low[1], env.action_space.high[1], num=action_res[1])\n",
    "for i in range(action_res[0]):\n",
    "    for j in range(action_res[1]):\n",
    "        s = action_res[1] * i + j\n",
    "        action_map[s, :] = [u[i], v[j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpReplay(object):\n",
    "    def __init__(self, memory_size, state_dim, act_dim, batch_size):\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.state_dim = state_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.states = np.empty((self.memory_size, self.state_dim), dtype=np.float32)\n",
    "        self.actions = np.empty((self.memory_size, 1), dtype=np.float32)\n",
    "        self.rewards = np.empty(self.memory_size, dtype=np.float32)\n",
    "        self.next_states = np.empty((self.memory_size, self.state_dim), dtype=np.float32)\n",
    "        self.count = 0\n",
    "        self.current = 0\n",
    "    def fifo(self, state, action, reward, next_state):\n",
    "        self.states[self.current] = state\n",
    "        self.actions[self.current] = action\n",
    "        self.rewards[self.current] = reward\n",
    "        self.next_states[self.current] = next_state\n",
    "        self.current = (self.current + 1) % self.memory_size\n",
    "        self.count = self.count + 1\n",
    "    def add_trajectory(self, states, actions, rewards, next_states):\n",
    "        num = len(observes)\n",
    "        for i in range(0, num):\n",
    "            self.fifo(states[i], actions[i], rewards[i], next_states[i])\n",
    "    def sampling(self):\n",
    "        indexes = np.random.randint(min(self.count, self.memory_size), size=self.batch_size)\n",
    "        states = self.states[indexes]\n",
    "        actions = self.actions[indexes]\n",
    "        rewards = self.rewards[indexes]\n",
    "        next_states = self.next_states[indexes]\n",
    "        states = states.reshape((-1, self.state_dim))\n",
    "        actions = actions.reshape((-1, 1))\n",
    "        rewards = rewards.reshape((-1, 1))\n",
    "        next_states = next_states.reshape((-1, self.state_dim))\n",
    "        return states, actions, rewards, next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    def __init__(self, memory_size, state_dim, act_dim):\n",
    "        self.memory_size = memory_size\n",
    "        self.state_dim = state_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.states = np.empty((self.memory_size, self.state_dim), dtype=np.float32)\n",
    "        self.actions = np.empty((self.memory_size, 1), dtype=np.float32)\n",
    "        self.rewards = np.empty(self.memory_size, dtype=np.float32)\n",
    "        self.next_states = np.empty((self.memory_size, self.state_dim), dtype=np.float32)\n",
    "        self.count = 0\n",
    "        self.current = 0\n",
    "    def fifo(self, state, action, reward, next_state):\n",
    "        self.states[self.current] = state\n",
    "        self.actions[self.current] = action\n",
    "        self.rewards[self.current] = reward\n",
    "        self.next_states[self.current] = next_state\n",
    "        self.current = (self.current + 1) % self.memory_size\n",
    "        self.count = self.count + 1\n",
    "    def add_trajectory(self, states, actions, rewards, next_states):\n",
    "        num = len(observes)\n",
    "        for i in range(0, num):\n",
    "            self.fifo(states[i], actions[i], rewards[i], next_states[i])\n",
    "    def sampling(self):\n",
    "        randpermlist = np.random.permutation(self.count)\n",
    "        states = self.states[randpermlist]\n",
    "        actions = self.actions[randpermlist]\n",
    "        rewards = self.rewards[randpermlist]\n",
    "        next_states = self.next_states[randpermlist]\n",
    "        states = states.reshape((-1, self.state_dim))\n",
    "        actions = actions.reshape((-1, 1))\n",
    "        rewards = rewards.reshape((-1, 1))\n",
    "        next_states = next_states.reshape((-1, self.state_dim))\n",
    "        return states, actions, rewards, next_states\n",
    "    def empty_memory(self):\n",
    "        self.states = np.empty((self.memory_size, self.state_dim), dtype=np.float32)\n",
    "        self.actions = np.empty((self.memory_size, 1), dtype=np.float32)\n",
    "        self.rewards = np.empty(self.memory_size, dtype=np.float32)\n",
    "        self.next_states = np.empty((self.memory_size, self.state_dim), dtype=np.float32)\n",
    "        self.count = 0\n",
    "        self.current = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(object):\n",
    "    def __init__(self, sess, n_features, n_actions, batch_size, train_itr, eps_decay_rate, lr=0.0005):\n",
    "        \n",
    "        self.sess = sess\n",
    "        self.batch_size = batch_size\n",
    "        self.train_itr = train_itr\n",
    "        self.s = tf.placeholder(tf.float32, (None, n_features), \"state\")\n",
    "        self.a = tf.placeholder(tf.float32, (None, n_actions), \"act\")\n",
    "        self.td_error = tf.placeholder(tf.float32, (None, 1), \"td_error\")  # TD_error]\n",
    "        self.eps = 1\n",
    "        self.eps_decay_rate = eps_decay_rate\n",
    "        self.eps_min = 0\n",
    "\n",
    "        with tf.variable_scope('Actor'):\n",
    "            l1 = tf.layers.dense(\n",
    "                inputs=self.s,\n",
    "                units=64,    # number of hidden units\n",
    "                activation=tf.tanh,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),    # weights\n",
    "                # bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l1'\n",
    "            )\n",
    "\n",
    "            l2 = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=32,    # number of hidden units\n",
    "                activation=tf.tanh,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),    # weights\n",
    "                # bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l2'\n",
    "            )\n",
    "            \n",
    "            self.acts_prob = tf.layers.dense(\n",
    "                inputs=l2,\n",
    "                units=n_actions,    # output units\n",
    "                activation=tf.contrib.sparsemax.sparsemax,   # get action probabilities\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                # bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='acts_prob'\n",
    "            )\n",
    "            \n",
    "        with tf.variable_scope('exp_v'):\n",
    "            self.error = tf.maximum(self.td_error/ALPHA + 0.5 + \n",
    "                             0.5*tf.reduce_sum(tf.multiply(self.acts_prob, self.acts_prob)\n",
    "                                               , 1, keep_dims=True), 0) - tf.reduce_sum(\n",
    "                tf.multiply(self.acts_prob, self.a), 1, keep_dims=True)\n",
    "            self.loss = tf.reduce_sum(tf.square(self.error))\n",
    "\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)  \n",
    "            \n",
    "    def learn(self, s, a, td):\n",
    "        BATCH_SIZE = self.batch_size\n",
    "        ITER = self.train_itr\n",
    "        for i in range(0, ITER):\n",
    "            randpermlist = np.random.permutation(s.shape[0])\n",
    "            s = s[randpermlist]\n",
    "            a = a[randpermlist]\n",
    "            td = td[randpermlist]\n",
    "            NITER = int(s.shape[0]/BATCH_SIZE)\n",
    "            for i in range(0, NITER):\n",
    "                feed_dict = {self.s: s[i*BATCH_SIZE:(i+1)*BATCH_SIZE], \n",
    "                             self.a: a[i*BATCH_SIZE:(i+1)*BATCH_SIZE], \n",
    "                             self.td_error: td[i*BATCH_SIZE:(i+1)*BATCH_SIZE]}\n",
    "                _, loss = self.sess.run([self.train_op, self.loss], feed_dict)\n",
    "        \n",
    "        return 0\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        s = s[np.newaxis, :]\n",
    "        probs = self.sess.run(self.acts_prob, {self.s: s})   # get probabilities for all actions\n",
    "        probs = probs.ravel()\n",
    "        probs = np.ones(probs.shape[0]) * self.eps / probs.shape[0] + probs * (1. - self.eps)\n",
    "        probs = probs/np.sum(probs)\n",
    "        return np.random.choice(np.arange(probs.shape[0]), p=probs)   # return a int\n",
    "    \n",
    "    def update_policy(self):\n",
    "        self.eps = np.max((self.eps*self.eps_decay_rate, self.eps_min))\n",
    "        \n",
    "    def print_prob(self, s):\n",
    "        probs = self.sess.run(self.acts_prob, {self.s: s})   # get probabilities for all actions\n",
    "        probs = probs.ravel()\n",
    "        probs = np.ones(probs.shape[0]) * self.eps / probs.shape[0] + probs * (1. - self.eps)\n",
    "        probs = probs/np.sum(probs)\n",
    "        print(probs)\n",
    "        return probs\n",
    "    \n",
    "    def get_probs(self, s, a):\n",
    "        probs = self.sess.run(self.acts_prob, {self.s: s})   # get probabilities for all actions\n",
    "        probs = np.multiply(probs, a)\n",
    "        probs = np.sum(probs, axis=1, keepdims=True)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(object):\n",
    "    def __init__(self, sess, n_features, batch_size, train_itr, lr=0.01):\n",
    "        \n",
    "        self.sess = sess\n",
    "        self.batch_size = batch_size\n",
    "        self.train_itr = train_itr\n",
    "        self.s = tf.placeholder(tf.float32, (None, n_features), \"state\")\n",
    "        self.v_ = tf.placeholder(tf.float32, (None, 1), \"v_next\")\n",
    "        self.r = tf.placeholder(tf.float32, (None, 1), 'r')\n",
    "        self.p = tf.placeholder(tf.float32, (None, 1), 'p')\n",
    "\n",
    "        with tf.variable_scope('Critic'):\n",
    "            l1 = tf.layers.dense(\n",
    "                inputs=self.s,\n",
    "                units=32,  # number of hidden units\n",
    "                activation=tf.tanh,  # None\n",
    "                # have to be linear to make sure the convergence of actor.\n",
    "                # But linear approximator seems hardly learns the correct Q.\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                # bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l1'\n",
    "            )\n",
    "            \n",
    "            l2 = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=16,  # number of hidden units\n",
    "                activation=tf.tanh,  # None\n",
    "                # have to be linear to make sure the convergence of actor.\n",
    "                # But linear approximator seems hardly learns the correct Q.\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                # bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l2'\n",
    "            )\n",
    "\n",
    "            self.v = tf.layers.dense(\n",
    "                inputs=l2,\n",
    "                units=1,  # output units\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                # bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='V'\n",
    "            )\n",
    "            \n",
    "        with tf.variable_scope('squared_TD_error'):\n",
    "            self.td_error = self.r + GAMMA * self.v_ - self.v\n",
    "            # self.loss = tf.reduce_sum(tf.square(self.td_error + ALPHA/2*(1 - self.p)))    # TD_error = (r+gamma*V_next) - V_eval\n",
    "            self.loss = tf.reduce_sum(tf.square(self.td_error))\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
    "\n",
    "    def learn(self, s, r, s_, p):\n",
    "        BATCH_SIZE = self.batch_size\n",
    "        ITER = self.train_itr\n",
    "        for i in range(0, ITER):\n",
    "            randpermlist = np.random.permutation(s.shape[0])\n",
    "            s = s[randpermlist]\n",
    "            r = r[randpermlist]\n",
    "            s_ = s_[randpermlist]\n",
    "            p = p[randpermlist]\n",
    "            NITER = int(s.shape[0]/BATCH_SIZE)\n",
    "            for i in range(0, NITER):\n",
    "                v_ = self.sess.run(self.v, {self.s: s_[i*BATCH_SIZE:(i+1)*BATCH_SIZE]})\n",
    "                td_error, _ = self.sess.run([self.td_error, self.train_op],\n",
    "                                                  {self.s: s[i*BATCH_SIZE:(i+1)*BATCH_SIZE], self.v_: v_, \n",
    "                                                   self.r: r[i*BATCH_SIZE:(i+1)*BATCH_SIZE],\n",
    "                                                   self.p: p[i*BATCH_SIZE:(i+1)*BATCH_SIZE]})\n",
    "        return 0\n",
    "    \n",
    "    def get_error(self, s, r, s_):\n",
    "        v_ = self.sess.run(self.v, {self.s: s_})\n",
    "        td_error = self.sess.run(self.td_error,\n",
    "                                          {self.s: s, self.v_: v_, self.r: r})\n",
    "        return td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('episode:', 0, '  reward:', -141)\n",
      "('episode:', 10, '  reward:', -145)\n",
      "('episode:', 20, '  reward:', -149)\n",
      "('episode:', 30, '  reward:', -152)\n",
      "('episode:', 40, '  reward:', -151)\n",
      "('episode:', 50, '  reward:', -151)\n",
      "('episode:', 60, '  reward:', -153)\n",
      "('episode:', 70, '  reward:', -152)\n",
      "('episode:', 80, '  reward:', -152)\n",
      "('episode:', 90, '  reward:', -154)\n",
      "('episode:', 100, '  reward:', -152)\n",
      "('episode:', 110, '  reward:', -152)\n",
      "('episode:', 120, '  reward:', -154)\n",
      "('episode:', 130, '  reward:', -155)\n",
      "('episode:', 140, '  reward:', -154)\n",
      "('episode:', 150, '  reward:', -154)\n",
      "('episode:', 160, '  reward:', -155)\n",
      "('episode:', 170, '  reward:', -155)\n",
      "('episode:', 180, '  reward:', -154)\n",
      "('episode:', 190, '  reward:', -154)\n",
      "('episode:', 200, '  reward:', -154)\n",
      "('episode:', 210, '  reward:', -154)\n",
      "('episode:', 220, '  reward:', -154)\n",
      "('episode:', 230, '  reward:', -153)\n",
      "('episode:', 240, '  reward:', -152)\n",
      "('episode:', 250, '  reward:', -153)\n",
      "('episode:', 260, '  reward:', -152)\n",
      "('episode:', 270, '  reward:', -152)\n",
      "('episode:', 280, '  reward:', -151)\n",
      "('episode:', 290, '  reward:', -153)\n",
      "('episode:', 300, '  reward:', -155)\n",
      "('episode:', 310, '  reward:', -154)\n",
      "('episode:', 320, '  reward:', -151)\n",
      "('episode:', 330, '  reward:', -152)\n",
      "('episode:', 340, '  reward:', -152)\n",
      "('episode:', 350, '  reward:', -151)\n",
      "('episode:', 360, '  reward:', -152)\n",
      "('episode:', 370, '  reward:', -153)\n",
      "('episode:', 380, '  reward:', -152)\n",
      "('episode:', 390, '  reward:', -151)\n",
      "('episode:', 400, '  reward:', -151)\n",
      "('episode:', 410, '  reward:', -153)\n",
      "('episode:', 420, '  reward:', -153)\n",
      "('episode:', 430, '  reward:', -154)\n",
      "('episode:', 440, '  reward:', -152)\n",
      "('episode:', 450, '  reward:', -153)\n",
      "('episode:', 460, '  reward:', -153)\n",
      "('episode:', 470, '  reward:', -153)\n",
      "('episode:', 480, '  reward:', -153)\n",
      "('episode:', 490, '  reward:', -156)\n",
      "('episode:', 500, '  reward:', -155)\n",
      "('episode:', 510, '  reward:', -155)\n",
      "('episode:', 520, '  reward:', -153)\n",
      "('episode:', 530, '  reward:', -153)\n",
      "('episode:', 540, '  reward:', -154)\n",
      "('episode:', 550, '  reward:', -153)\n",
      "('episode:', 560, '  reward:', -153)\n",
      "('episode:', 570, '  reward:', -152)\n",
      "('episode:', 580, '  reward:', -152)\n",
      "('episode:', 590, '  reward:', -151)\n",
      "('episode:', 600, '  reward:', -151)\n",
      "('episode:', 610, '  reward:', -151)\n",
      "('episode:', 620, '  reward:', -150)\n",
      "('episode:', 630, '  reward:', -151)\n",
      "('episode:', 640, '  reward:', -151)\n",
      "('episode:', 650, '  reward:', -150)\n",
      "('episode:', 660, '  reward:', -152)\n",
      "('episode:', 670, '  reward:', -153)\n",
      "('episode:', 680, '  reward:', -151)\n",
      "('episode:', 690, '  reward:', -152)\n",
      "('episode:', 700, '  reward:', -153)\n",
      "('episode:', 710, '  reward:', -151)\n",
      "('episode:', 720, '  reward:', -153)\n",
      "('episode:', 730, '  reward:', -153)\n",
      "('episode:', 740, '  reward:', -154)\n",
      "('episode:', 750, '  reward:', -154)\n",
      "('episode:', 760, '  reward:', -153)\n",
      "('episode:', 770, '  reward:', -153)\n",
      "('episode:', 780, '  reward:', -153)\n",
      "('episode:', 790, '  reward:', -152)\n",
      "('episode:', 800, '  reward:', -153)\n",
      "('episode:', 810, '  reward:', -153)\n",
      "('episode:', 820, '  reward:', -153)\n",
      "('episode:', 830, '  reward:', -152)\n",
      "('episode:', 840, '  reward:', -151)\n",
      "('episode:', 850, '  reward:', -152)\n",
      "('episode:', 860, '  reward:', -153)\n",
      "('episode:', 870, '  reward:', -152)\n",
      "('episode:', 880, '  reward:', -153)\n",
      "('episode:', 890, '  reward:', -154)\n",
      "('episode:', 900, '  reward:', -153)\n",
      "('episode:', 910, '  reward:', -154)\n",
      "('episode:', 920, '  reward:', -153)\n",
      "('episode:', 930, '  reward:', -152)\n",
      "('episode:', 940, '  reward:', -152)\n",
      "('episode:', 950, '  reward:', -151)\n",
      "('episode:', 960, '  reward:', -152)\n",
      "('episode:', 970, '  reward:', -151)\n",
      "('episode:', 980, '  reward:', -153)\n",
      "('episode:', 990, '  reward:', -151)\n",
      "('episode:', 1000, '  reward:', -152)\n",
      "('episode:', 1010, '  reward:', -152)\n",
      "('episode:', 1020, '  reward:', -151)\n",
      "('episode:', 1030, '  reward:', -151)\n",
      "('episode:', 1040, '  reward:', -152)\n",
      "('episode:', 1050, '  reward:', -153)\n",
      "('episode:', 1060, '  reward:', -152)\n",
      "('episode:', 1070, '  reward:', -151)\n",
      "('episode:', 1080, '  reward:', -151)\n",
      "('episode:', 1090, '  reward:', -151)\n",
      "('episode:', 1100, '  reward:', -151)\n",
      "('episode:', 1110, '  reward:', -153)\n",
      "('episode:', 1120, '  reward:', -153)\n",
      "('episode:', 1130, '  reward:', -151)\n",
      "('episode:', 1140, '  reward:', -153)\n",
      "('episode:', 1150, '  reward:', -154)\n",
      "('episode:', 1160, '  reward:', -153)\n",
      "('episode:', 1170, '  reward:', -153)\n",
      "('episode:', 1180, '  reward:', -152)\n",
      "('episode:', 1190, '  reward:', -153)\n",
      "('episode:', 1200, '  reward:', -154)\n",
      "('episode:', 1210, '  reward:', -155)\n",
      "('episode:', 1220, '  reward:', -154)\n",
      "('episode:', 1230, '  reward:', -153)\n",
      "('episode:', 1240, '  reward:', -154)\n",
      "('episode:', 1250, '  reward:', -154)\n",
      "('episode:', 1260, '  reward:', -154)\n",
      "('episode:', 1270, '  reward:', -154)\n",
      "('episode:', 1280, '  reward:', -155)\n",
      "('episode:', 1290, '  reward:', -155)\n",
      "('episode:', 1300, '  reward:', -152)\n",
      "('episode:', 1310, '  reward:', -151)\n",
      "('episode:', 1320, '  reward:', -150)\n",
      "('episode:', 1330, '  reward:', -153)\n",
      "('episode:', 1340, '  reward:', -152)\n",
      "('episode:', 1350, '  reward:', -153)\n",
      "('episode:', 1360, '  reward:', -153)\n",
      "('episode:', 1370, '  reward:', -152)\n",
      "('episode:', 1380, '  reward:', -154)\n",
      "('episode:', 1390, '  reward:', -153)\n",
      "('episode:', 1400, '  reward:', -153)\n",
      "('episode:', 1410, '  reward:', -151)\n",
      "('episode:', 1420, '  reward:', -151)\n",
      "('episode:', 1430, '  reward:', -153)\n",
      "('episode:', 1440, '  reward:', -152)\n",
      "('episode:', 1450, '  reward:', -153)\n",
      "('episode:', 1460, '  reward:', -154)\n",
      "('episode:', 1470, '  reward:', -154)\n",
      "('episode:', 1480, '  reward:', -154)\n",
      "('episode:', 1490, '  reward:', -153)\n",
      "('episode:', 1500, '  reward:', -152)\n",
      "('episode:', 1510, '  reward:', -152)\n",
      "('episode:', 1520, '  reward:', -152)\n",
      "('episode:', 1530, '  reward:', -152)\n",
      "('episode:', 1540, '  reward:', -150)\n",
      "('episode:', 1550, '  reward:', -152)\n",
      "('episode:', 1560, '  reward:', -153)\n",
      "('episode:', 1570, '  reward:', -152)\n",
      "('episode:', 1580, '  reward:', -150)\n",
      "('episode:', 1590, '  reward:', -151)\n",
      "('episode:', 1600, '  reward:', -152)\n",
      "('episode:', 1610, '  reward:', -152)\n",
      "('episode:', 1620, '  reward:', -153)\n",
      "('episode:', 1630, '  reward:', -154)\n",
      "('episode:', 1640, '  reward:', -152)\n",
      "('episode:', 1650, '  reward:', -151)\n",
      "('episode:', 1660, '  reward:', -153)\n",
      "('episode:', 1670, '  reward:', -154)\n",
      "('episode:', 1680, '  reward:', -152)\n",
      "('episode:', 1690, '  reward:', -152)\n",
      "('episode:', 1700, '  reward:', -152)\n",
      "('episode:', 1710, '  reward:', -154)\n",
      "('episode:', 1720, '  reward:', -154)\n",
      "('episode:', 1730, '  reward:', -153)\n",
      "('episode:', 1740, '  reward:', -153)\n",
      "('episode:', 1750, '  reward:', -153)\n",
      "('episode:', 1760, '  reward:', -153)\n",
      "('episode:', 1770, '  reward:', -154)\n",
      "('episode:', 1780, '  reward:', -154)\n",
      "('episode:', 1790, '  reward:', -153)\n",
      "('episode:', 1800, '  reward:', -152)\n",
      "('episode:', 1810, '  reward:', -153)\n",
      "('episode:', 1820, '  reward:', -153)\n",
      "('episode:', 1830, '  reward:', -149)\n",
      "('episode:', 1840, '  reward:', -150)\n",
      "('episode:', 1850, '  reward:', -151)\n",
      "('episode:', 1860, '  reward:', -152)\n",
      "('episode:', 1870, '  reward:', -154)\n",
      "('episode:', 1880, '  reward:', -156)\n",
      "('episode:', 1890, '  reward:', -154)\n",
      "('episode:', 1900, '  reward:', -153)\n",
      "('episode:', 1910, '  reward:', -152)\n",
      "('episode:', 1920, '  reward:', -153)\n",
      "('episode:', 1930, '  reward:', -153)\n",
      "('episode:', 1940, '  reward:', -151)\n",
      "('episode:', 1950, '  reward:', -153)\n",
      "('episode:', 1960, '  reward:', -153)\n",
      "('episode:', 1970, '  reward:', -153)\n",
      "('episode:', 1980, '  reward:', -154)\n",
      "('episode:', 1990, '  reward:', -154)\n",
      "('episode:', 2000, '  reward:', -156)\n",
      "('episode:', 2010, '  reward:', -153)\n",
      "('episode:', 2020, '  reward:', -153)\n",
      "('episode:', 2030, '  reward:', -154)\n",
      "('episode:', 2040, '  reward:', -153)\n",
      "('episode:', 2050, '  reward:', -151)\n",
      "('episode:', 2060, '  reward:', -151)\n",
      "('episode:', 2070, '  reward:', -150)\n",
      "('episode:', 2080, '  reward:', -152)\n",
      "('episode:', 2090, '  reward:', -152)\n",
      "('episode:', 2100, '  reward:', -150)\n",
      "('episode:', 2110, '  reward:', -151)\n",
      "('episode:', 2120, '  reward:', -152)\n",
      "('episode:', 2130, '  reward:', -151)\n",
      "('episode:', 2140, '  reward:', -151)\n",
      "('episode:', 2150, '  reward:', -152)\n",
      "('episode:', 2160, '  reward:', -151)\n",
      "('episode:', 2170, '  reward:', -151)\n",
      "('episode:', 2180, '  reward:', -152)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('episode:', 2190, '  reward:', -152)\n",
      "('episode:', 2200, '  reward:', -152)\n",
      "('episode:', 2210, '  reward:', -151)\n",
      "('episode:', 2220, '  reward:', -152)\n",
      "('episode:', 2230, '  reward:', -152)\n",
      "('episode:', 2240, '  reward:', -152)\n",
      "('episode:', 2250, '  reward:', -154)\n",
      "('episode:', 2260, '  reward:', -153)\n",
      "('episode:', 2270, '  reward:', -152)\n",
      "('episode:', 2280, '  reward:', -152)\n",
      "('episode:', 2290, '  reward:', -151)\n",
      "('episode:', 2300, '  reward:', -152)\n",
      "('episode:', 2310, '  reward:', -153)\n",
      "('episode:', 2320, '  reward:', -154)\n",
      "('episode:', 2330, '  reward:', -153)\n",
      "('episode:', 2340, '  reward:', -150)\n",
      "('episode:', 2350, '  reward:', -152)\n",
      "('episode:', 2360, '  reward:', -152)\n",
      "('episode:', 2370, '  reward:', -153)\n",
      "('episode:', 2380, '  reward:', -152)\n",
      "('episode:', 2390, '  reward:', -150)\n",
      "('episode:', 2400, '  reward:', -151)\n",
      "('episode:', 2410, '  reward:', -152)\n",
      "('episode:', 2420, '  reward:', -152)\n",
      "('episode:', 2430, '  reward:', -153)\n",
      "('episode:', 2440, '  reward:', -152)\n",
      "('episode:', 2450, '  reward:', -152)\n",
      "('episode:', 2460, '  reward:', -151)\n",
      "('episode:', 2470, '  reward:', -152)\n",
      "('episode:', 2480, '  reward:', -153)\n",
      "('episode:', 2490, '  reward:', -153)\n",
      "('episode:', 2500, '  reward:', -154)\n",
      "('episode:', 2510, '  reward:', -154)\n",
      "('episode:', 2520, '  reward:', -155)\n",
      "('episode:', 2530, '  reward:', -152)\n",
      "('episode:', 2540, '  reward:', -151)\n",
      "('episode:', 2550, '  reward:', -153)\n",
      "('episode:', 2560, '  reward:', -152)\n",
      "('episode:', 2570, '  reward:', -152)\n",
      "('episode:', 2580, '  reward:', -154)\n",
      "('episode:', 2590, '  reward:', -152)\n",
      "('episode:', 2600, '  reward:', -153)\n",
      "('episode:', 2610, '  reward:', -154)\n",
      "('episode:', 2620, '  reward:', -154)\n",
      "('episode:', 2630, '  reward:', -155)\n",
      "('episode:', 2640, '  reward:', -155)\n",
      "('episode:', 2650, '  reward:', -157)\n",
      "('episode:', 2660, '  reward:', -157)\n",
      "('episode:', 2670, '  reward:', -155)\n",
      "('episode:', 2680, '  reward:', -154)\n",
      "('episode:', 2690, '  reward:', -151)\n",
      "('episode:', 2700, '  reward:', -151)\n",
      "('episode:', 2710, '  reward:', -151)\n",
      "('episode:', 2720, '  reward:', -153)\n",
      "('episode:', 2730, '  reward:', -152)\n",
      "('episode:', 2740, '  reward:', -152)\n",
      "('episode:', 2750, '  reward:', -151)\n",
      "('episode:', 2760, '  reward:', -152)\n",
      "('episode:', 2770, '  reward:', -150)\n",
      "('episode:', 2780, '  reward:', -151)\n",
      "('episode:', 2790, '  reward:', -153)\n",
      "('episode:', 2800, '  reward:', -152)\n",
      "('episode:', 2810, '  reward:', -152)\n",
      "('episode:', 2820, '  reward:', -153)\n",
      "('episode:', 2830, '  reward:', -154)\n",
      "('episode:', 2840, '  reward:', -154)\n",
      "('episode:', 2850, '  reward:', -155)\n",
      "('episode:', 2860, '  reward:', -153)\n",
      "('episode:', 2870, '  reward:', -152)\n",
      "('episode:', 2880, '  reward:', -150)\n",
      "('episode:', 2890, '  reward:', -151)\n",
      "('episode:', 2900, '  reward:', -151)\n",
      "('episode:', 2910, '  reward:', -152)\n",
      "('episode:', 2920, '  reward:', -154)\n",
      "('episode:', 2930, '  reward:', -152)\n",
      "('episode:', 2940, '  reward:', -153)\n",
      "('episode:', 2950, '  reward:', -153)\n",
      "('episode:', 2960, '  reward:', -151)\n",
      "('episode:', 2970, '  reward:', -150)\n",
      "('episode:', 2980, '  reward:', -150)\n",
      "('episode:', 2990, '  reward:', -152)\n",
      "('episode:', 3000, '  reward:', -153)\n",
      "('episode:', 3010, '  reward:', -152)\n",
      "('episode:', 3020, '  reward:', -154)\n",
      "('episode:', 3030, '  reward:', -154)\n",
      "('episode:', 3040, '  reward:', -154)\n",
      "('episode:', 3050, '  reward:', -152)\n",
      "('episode:', 3060, '  reward:', -153)\n",
      "('episode:', 3070, '  reward:', -153)\n",
      "('episode:', 3080, '  reward:', -153)\n",
      "('episode:', 3090, '  reward:', -152)\n",
      "('episode:', 3100, '  reward:', -153)\n",
      "('episode:', 3110, '  reward:', -153)\n",
      "('episode:', 3120, '  reward:', -150)\n",
      "('episode:', 3130, '  reward:', -149)\n",
      "('episode:', 3140, '  reward:', -149)\n",
      "('episode:', 3150, '  reward:', -150)\n",
      "('episode:', 3160, '  reward:', -151)\n",
      "('episode:', 3170, '  reward:', -151)\n",
      "('episode:', 3180, '  reward:', -151)\n",
      "('episode:', 3190, '  reward:', -152)\n",
      "('episode:', 3200, '  reward:', -152)\n",
      "('episode:', 3210, '  reward:', -153)\n",
      "('episode:', 3220, '  reward:', -151)\n",
      "('episode:', 3230, '  reward:', -151)\n",
      "('episode:', 3240, '  reward:', -152)\n",
      "('episode:', 3250, '  reward:', -153)\n",
      "('episode:', 3260, '  reward:', -153)\n",
      "('episode:', 3270, '  reward:', -153)\n",
      "('episode:', 3280, '  reward:', -153)\n",
      "('episode:', 3290, '  reward:', -152)\n",
      "('episode:', 3300, '  reward:', -152)\n",
      "('episode:', 3310, '  reward:', -150)\n",
      "('episode:', 3320, '  reward:', -150)\n",
      "('episode:', 3330, '  reward:', -151)\n",
      "('episode:', 3340, '  reward:', -152)\n",
      "('episode:', 3350, '  reward:', -151)\n",
      "('episode:', 3360, '  reward:', -153)\n",
      "('episode:', 3370, '  reward:', -152)\n",
      "('episode:', 3380, '  reward:', -153)\n",
      "('episode:', 3390, '  reward:', -152)\n",
      "('episode:', 3400, '  reward:', -152)\n",
      "('episode:', 3410, '  reward:', -152)\n",
      "('episode:', 3420, '  reward:', -151)\n",
      "('episode:', 3430, '  reward:', -152)\n",
      "('episode:', 3440, '  reward:', -154)\n",
      "('episode:', 3450, '  reward:', -153)\n",
      "('episode:', 3460, '  reward:', -151)\n",
      "('episode:', 3470, '  reward:', -154)\n",
      "('episode:', 3480, '  reward:', -153)\n",
      "('episode:', 3490, '  reward:', -153)\n",
      "('episode:', 3500, '  reward:', -151)\n",
      "('episode:', 3510, '  reward:', -151)\n",
      "('episode:', 3520, '  reward:', -152)\n",
      "('episode:', 3530, '  reward:', -151)\n",
      "('episode:', 3540, '  reward:', -150)\n",
      "('episode:', 3550, '  reward:', -151)\n",
      "('episode:', 3560, '  reward:', -150)\n",
      "('episode:', 3570, '  reward:', -151)\n",
      "('episode:', 3580, '  reward:', -153)\n",
      "('episode:', 3590, '  reward:', -153)\n",
      "('episode:', 3600, '  reward:', -153)\n",
      "('episode:', 3610, '  reward:', -155)\n",
      "('episode:', 3620, '  reward:', -153)\n",
      "('episode:', 3630, '  reward:', -153)\n",
      "('episode:', 3640, '  reward:', -152)\n",
      "('episode:', 3650, '  reward:', -152)\n",
      "('episode:', 3660, '  reward:', -152)\n",
      "('episode:', 3670, '  reward:', -152)\n",
      "('episode:', 3680, '  reward:', -152)\n",
      "('episode:', 3690, '  reward:', -154)\n",
      "('episode:', 3700, '  reward:', -155)\n",
      "('episode:', 3710, '  reward:', -156)\n",
      "('episode:', 3720, '  reward:', -155)\n",
      "('episode:', 3730, '  reward:', -154)\n",
      "('episode:', 3740, '  reward:', -153)\n",
      "('episode:', 3750, '  reward:', -152)\n",
      "('episode:', 3760, '  reward:', -152)\n",
      "('episode:', 3770, '  reward:', -154)\n",
      "('episode:', 3780, '  reward:', -152)\n",
      "('episode:', 3790, '  reward:', -152)\n",
      "('episode:', 3800, '  reward:', -151)\n",
      "('episode:', 3810, '  reward:', -152)\n",
      "('episode:', 3820, '  reward:', -152)\n",
      "('episode:', 3830, '  reward:', -152)\n",
      "('episode:', 3840, '  reward:', -153)\n",
      "('episode:', 3850, '  reward:', -153)\n",
      "('episode:', 3860, '  reward:', -153)\n",
      "('episode:', 3870, '  reward:', -153)\n",
      "('episode:', 3880, '  reward:', -153)\n",
      "('episode:', 3890, '  reward:', -152)\n",
      "('episode:', 3900, '  reward:', -153)\n",
      "('episode:', 3910, '  reward:', -153)\n",
      "('episode:', 3920, '  reward:', -151)\n",
      "('episode:', 3930, '  reward:', -151)\n",
      "('episode:', 3940, '  reward:', -151)\n",
      "('episode:', 3950, '  reward:', -153)\n",
      "('episode:', 3960, '  reward:', -152)\n",
      "('episode:', 3970, '  reward:', -153)\n",
      "('episode:', 3980, '  reward:', -153)\n",
      "('episode:', 3990, '  reward:', -151)\n",
      "('episode:', 4000, '  reward:', -152)\n",
      "('episode:', 4010, '  reward:', -153)\n",
      "('episode:', 4020, '  reward:', -152)\n",
      "('episode:', 4030, '  reward:', -151)\n",
      "('episode:', 4040, '  reward:', -151)\n",
      "('episode:', 4050, '  reward:', -152)\n",
      "('episode:', 4060, '  reward:', -152)\n",
      "('episode:', 4070, '  reward:', -153)\n",
      "('episode:', 4080, '  reward:', -152)\n",
      "('episode:', 4090, '  reward:', -153)\n",
      "('episode:', 4100, '  reward:', -153)\n",
      "('episode:', 4110, '  reward:', -152)\n",
      "('episode:', 4120, '  reward:', -152)\n",
      "('episode:', 4130, '  reward:', -154)\n",
      "('episode:', 4140, '  reward:', -152)\n",
      "('episode:', 4150, '  reward:', -150)\n",
      "('episode:', 4160, '  reward:', -150)\n",
      "('episode:', 4170, '  reward:', -152)\n",
      "('episode:', 4180, '  reward:', -152)\n",
      "('episode:', 4190, '  reward:', -152)\n",
      "('episode:', 4200, '  reward:', -151)\n",
      "('episode:', 4210, '  reward:', -150)\n",
      "('episode:', 4220, '  reward:', -151)\n",
      "('episode:', 4230, '  reward:', -151)\n",
      "('episode:', 4240, '  reward:', -150)\n",
      "('episode:', 4250, '  reward:', -150)\n",
      "('episode:', 4260, '  reward:', -151)\n",
      "('episode:', 4270, '  reward:', -153)\n",
      "('episode:', 4280, '  reward:', -151)\n",
      "('episode:', 4290, '  reward:', -151)\n",
      "('episode:', 4300, '  reward:', -149)\n",
      "('episode:', 4310, '  reward:', -152)\n",
      "('episode:', 4320, '  reward:', -153)\n",
      "('episode:', 4330, '  reward:', -152)\n",
      "('episode:', 4340, '  reward:', -152)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('episode:', 4350, '  reward:', -151)\n",
      "('episode:', 4360, '  reward:', -151)\n",
      "('episode:', 4370, '  reward:', -150)\n",
      "('episode:', 4380, '  reward:', -151)\n",
      "('episode:', 4390, '  reward:', -152)\n",
      "('episode:', 4400, '  reward:', -152)\n",
      "('episode:', 4410, '  reward:', -152)\n",
      "('episode:', 4420, '  reward:', -153)\n",
      "('episode:', 4430, '  reward:', -154)\n",
      "('episode:', 4440, '  reward:', -154)\n",
      "('episode:', 4450, '  reward:', -152)\n",
      "('episode:', 4460, '  reward:', -152)\n",
      "('episode:', 4470, '  reward:', -152)\n",
      "('episode:', 4480, '  reward:', -152)\n",
      "('episode:', 4490, '  reward:', -153)\n",
      "('episode:', 4500, '  reward:', -154)\n",
      "('episode:', 4510, '  reward:', -154)\n",
      "('episode:', 4520, '  reward:', -154)\n",
      "('episode:', 4530, '  reward:', -153)\n",
      "('episode:', 4540, '  reward:', -152)\n",
      "('episode:', 4550, '  reward:', -152)\n",
      "('episode:', 4560, '  reward:', -149)\n",
      "('episode:', 4570, '  reward:', -152)\n",
      "('episode:', 4580, '  reward:', -152)\n",
      "('episode:', 4590, '  reward:', -153)\n",
      "('episode:', 4600, '  reward:', -151)\n",
      "('episode:', 4610, '  reward:', -152)\n",
      "('episode:', 4620, '  reward:', -152)\n",
      "('episode:', 4630, '  reward:', -153)\n",
      "('episode:', 4640, '  reward:', -151)\n",
      "('episode:', 4650, '  reward:', -152)\n",
      "('episode:', 4660, '  reward:', -152)\n",
      "('episode:', 4670, '  reward:', -151)\n",
      "('episode:', 4680, '  reward:', -152)\n",
      "('episode:', 4690, '  reward:', -153)\n",
      "('episode:', 4700, '  reward:', -153)\n",
      "('episode:', 4710, '  reward:', -153)\n",
      "('episode:', 4720, '  reward:', -152)\n",
      "('episode:', 4730, '  reward:', -152)\n",
      "('episode:', 4740, '  reward:', -152)\n",
      "('episode:', 4750, '  reward:', -151)\n",
      "('episode:', 4760, '  reward:', -153)\n",
      "('episode:', 4770, '  reward:', -154)\n",
      "('episode:', 4780, '  reward:', -155)\n",
      "('episode:', 4790, '  reward:', -153)\n",
      "('episode:', 4800, '  reward:', -151)\n",
      "('episode:', 4810, '  reward:', -151)\n",
      "('episode:', 4820, '  reward:', -152)\n",
      "('episode:', 4830, '  reward:', -153)\n",
      "('episode:', 4840, '  reward:', -152)\n",
      "('episode:', 4850, '  reward:', -153)\n",
      "('episode:', 4860, '  reward:', -153)\n",
      "('episode:', 4870, '  reward:', -153)\n",
      "('episode:', 4880, '  reward:', -152)\n",
      "('episode:', 4890, '  reward:', -152)\n",
      "('episode:', 4900, '  reward:', -154)\n",
      "('episode:', 4910, '  reward:', -155)\n",
      "('episode:', 4920, '  reward:', -154)\n",
      "('episode:', 4930, '  reward:', -152)\n",
      "('episode:', 4940, '  reward:', -150)\n",
      "('episode:', 4950, '  reward:', -150)\n",
      "('episode:', 4960, '  reward:', -151)\n",
      "('episode:', 4970, '  reward:', -151)\n",
      "('episode:', 4980, '  reward:', -153)\n",
      "('episode:', 4990, '  reward:', -153)\n",
      "('episode:', 5000, '  reward:', -153)\n",
      "('episode:', 5010, '  reward:', -154)\n",
      "('episode:', 5020, '  reward:', -154)\n",
      "('episode:', 5030, '  reward:', -154)\n",
      "('episode:', 5040, '  reward:', -153)\n",
      "('episode:', 5050, '  reward:', -152)\n",
      "('episode:', 5060, '  reward:', -151)\n",
      "('episode:', 5070, '  reward:', -152)\n",
      "('episode:', 5080, '  reward:', -151)\n",
      "('episode:', 5090, '  reward:', -152)\n",
      "('episode:', 5100, '  reward:', -151)\n",
      "('episode:', 5110, '  reward:', -150)\n",
      "('episode:', 5120, '  reward:', -153)\n",
      "('episode:', 5130, '  reward:', -154)\n",
      "('episode:', 5140, '  reward:', -152)\n",
      "('episode:', 5150, '  reward:', -153)\n",
      "('episode:', 5160, '  reward:', -151)\n",
      "('episode:', 5170, '  reward:', -151)\n",
      "('episode:', 5180, '  reward:', -150)\n",
      "('episode:', 5190, '  reward:', -151)\n",
      "('episode:', 5200, '  reward:', -153)\n",
      "('episode:', 5210, '  reward:', -153)\n",
      "('episode:', 5220, '  reward:', -153)\n",
      "('episode:', 5230, '  reward:', -153)\n",
      "('episode:', 5240, '  reward:', -152)\n",
      "('episode:', 5250, '  reward:', -151)\n",
      "('episode:', 5260, '  reward:', -152)\n",
      "('episode:', 5270, '  reward:', -153)\n",
      "('episode:', 5280, '  reward:', -152)\n",
      "('episode:', 5290, '  reward:', -151)\n",
      "('episode:', 5300, '  reward:', -152)\n",
      "('episode:', 5310, '  reward:', -151)\n",
      "('episode:', 5320, '  reward:', -151)\n",
      "('episode:', 5330, '  reward:', -151)\n",
      "('episode:', 5340, '  reward:', -151)\n",
      "('episode:', 5350, '  reward:', -150)\n",
      "('episode:', 5360, '  reward:', -151)\n",
      "('episode:', 5370, '  reward:', -151)\n",
      "('episode:', 5380, '  reward:', -151)\n",
      "('episode:', 5390, '  reward:', -153)\n",
      "('episode:', 5400, '  reward:', -152)\n",
      "('episode:', 5410, '  reward:', -152)\n",
      "('episode:', 5420, '  reward:', -151)\n",
      "('episode:', 5430, '  reward:', -151)\n",
      "('episode:', 5440, '  reward:', -151)\n",
      "('episode:', 5450, '  reward:', -154)\n",
      "('episode:', 5460, '  reward:', -154)\n",
      "('episode:', 5470, '  reward:', -152)\n",
      "('episode:', 5480, '  reward:', -152)\n",
      "('episode:', 5490, '  reward:', -154)\n",
      "('episode:', 5500, '  reward:', -152)\n",
      "('episode:', 5510, '  reward:', -152)\n",
      "('episode:', 5520, '  reward:', -152)\n",
      "('episode:', 5530, '  reward:', -150)\n",
      "('episode:', 5540, '  reward:', -152)\n",
      "('episode:', 5550, '  reward:', -151)\n",
      "('episode:', 5560, '  reward:', -152)\n",
      "('episode:', 5570, '  reward:', -154)\n",
      "('episode:', 5580, '  reward:', -151)\n",
      "('episode:', 5590, '  reward:', -150)\n",
      "('episode:', 5600, '  reward:', -151)\n",
      "('episode:', 5610, '  reward:', -150)\n",
      "('episode:', 5620, '  reward:', -152)\n",
      "('episode:', 5630, '  reward:', -151)\n",
      "('episode:', 5640, '  reward:', -150)\n",
      "('episode:', 5650, '  reward:', -149)\n",
      "('episode:', 5660, '  reward:', -149)\n",
      "('episode:', 5670, '  reward:', -150)\n",
      "('episode:', 5680, '  reward:', -151)\n",
      "('episode:', 5690, '  reward:', -151)\n",
      "('episode:', 5700, '  reward:', -152)\n",
      "('episode:', 5710, '  reward:', -153)\n",
      "('episode:', 5720, '  reward:', -152)\n",
      "('episode:', 5730, '  reward:', -155)\n",
      "('episode:', 5740, '  reward:', -154)\n",
      "('episode:', 5750, '  reward:', -153)\n",
      "('episode:', 5760, '  reward:', -152)\n",
      "('episode:', 5770, '  reward:', -152)\n",
      "('episode:', 5780, '  reward:', -153)\n",
      "('episode:', 5790, '  reward:', -153)\n",
      "('episode:', 5800, '  reward:', -151)\n",
      "('episode:', 5810, '  reward:', -152)\n",
      "('episode:', 5820, '  reward:', -152)\n",
      "('episode:', 5830, '  reward:', -153)\n",
      "('episode:', 5840, '  reward:', -152)\n",
      "('episode:', 5850, '  reward:', -151)\n",
      "('episode:', 5860, '  reward:', -152)\n",
      "('episode:', 5870, '  reward:', -151)\n",
      "('episode:', 5880, '  reward:', -151)\n",
      "('episode:', 5890, '  reward:', -152)\n",
      "('episode:', 5900, '  reward:', -153)\n",
      "('episode:', 5910, '  reward:', -152)\n",
      "('episode:', 5920, '  reward:', -151)\n",
      "('episode:', 5930, '  reward:', -149)\n",
      "('episode:', 5940, '  reward:', -150)\n",
      "('episode:', 5950, '  reward:', -151)\n",
      "('episode:', 5960, '  reward:', -153)\n",
      "('episode:', 5970, '  reward:', -151)\n",
      "('episode:', 5980, '  reward:', -149)\n",
      "('episode:', 5990, '  reward:', -153)\n",
      "('episode:', 6000, '  reward:', -152)\n",
      "('episode:', 6010, '  reward:', -153)\n",
      "('episode:', 6020, '  reward:', -153)\n",
      "('episode:', 6030, '  reward:', -154)\n",
      "('episode:', 6040, '  reward:', -152)\n",
      "('episode:', 6050, '  reward:', -155)\n",
      "('episode:', 6060, '  reward:', -156)\n",
      "('episode:', 6070, '  reward:', -153)\n",
      "('episode:', 6080, '  reward:', -152)\n",
      "('episode:', 6090, '  reward:', -152)\n",
      "('episode:', 6100, '  reward:', -152)\n",
      "('episode:', 6110, '  reward:', -152)\n",
      "('episode:', 6120, '  reward:', -153)\n",
      "('episode:', 6130, '  reward:', -153)\n",
      "('episode:', 6140, '  reward:', -153)\n",
      "('episode:', 6150, '  reward:', -153)\n",
      "('episode:', 6160, '  reward:', -152)\n",
      "('episode:', 6170, '  reward:', -152)\n",
      "('episode:', 6180, '  reward:', -152)\n",
      "('episode:', 6190, '  reward:', -151)\n",
      "('episode:', 6200, '  reward:', -151)\n",
      "('episode:', 6210, '  reward:', -152)\n",
      "('episode:', 6220, '  reward:', -151)\n",
      "('episode:', 6230, '  reward:', -151)\n",
      "('episode:', 6240, '  reward:', -152)\n",
      "('episode:', 6250, '  reward:', -151)\n",
      "('episode:', 6260, '  reward:', -154)\n",
      "('episode:', 6270, '  reward:', -153)\n",
      "('episode:', 6280, '  reward:', -151)\n",
      "('episode:', 6290, '  reward:', -152)\n",
      "('episode:', 6300, '  reward:', -151)\n",
      "('episode:', 6310, '  reward:', -153)\n",
      "('episode:', 6320, '  reward:', -152)\n",
      "('episode:', 6330, '  reward:', -153)\n",
      "('episode:', 6340, '  reward:', -153)\n",
      "('episode:', 6350, '  reward:', -154)\n",
      "('episode:', 6360, '  reward:', -152)\n",
      "('episode:', 6370, '  reward:', -152)\n",
      "('episode:', 6380, '  reward:', -152)\n",
      "('episode:', 6390, '  reward:', -152)\n",
      "('episode:', 6400, '  reward:', -153)\n",
      "('episode:', 6410, '  reward:', -153)\n",
      "('episode:', 6420, '  reward:', -151)\n",
      "('episode:', 6430, '  reward:', -153)\n",
      "('episode:', 6440, '  reward:', -154)\n",
      "('episode:', 6450, '  reward:', -153)\n",
      "('episode:', 6460, '  reward:', -152)\n",
      "('episode:', 6470, '  reward:', -151)\n",
      "('episode:', 6480, '  reward:', -151)\n",
      "('episode:', 6490, '  reward:', -150)\n",
      "('episode:', 6500, '  reward:', -150)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('episode:', 6510, '  reward:', -150)\n",
      "('episode:', 6520, '  reward:', -150)\n",
      "('episode:', 6530, '  reward:', -152)\n",
      "('episode:', 6540, '  reward:', -152)\n",
      "('episode:', 6550, '  reward:', -154)\n",
      "('episode:', 6560, '  reward:', -150)\n",
      "('episode:', 6570, '  reward:', -152)\n",
      "('episode:', 6580, '  reward:', -152)\n",
      "('episode:', 6590, '  reward:', -155)\n",
      "('episode:', 6600, '  reward:', -153)\n",
      "('episode:', 6610, '  reward:', -152)\n",
      "('episode:', 6620, '  reward:', -152)\n",
      "('episode:', 6630, '  reward:', -151)\n",
      "('episode:', 6640, '  reward:', -151)\n",
      "('episode:', 6650, '  reward:', -152)\n",
      "('episode:', 6660, '  reward:', -151)\n",
      "('episode:', 6670, '  reward:', -151)\n",
      "('episode:', 6680, '  reward:', -153)\n",
      "('episode:', 6690, '  reward:', -152)\n",
      "('episode:', 6700, '  reward:', -152)\n",
      "('episode:', 6710, '  reward:', -151)\n",
      "('episode:', 6720, '  reward:', -152)\n",
      "('episode:', 6730, '  reward:', -151)\n",
      "('episode:', 6740, '  reward:', -152)\n",
      "('episode:', 6750, '  reward:', -152)\n",
      "('episode:', 6760, '  reward:', -152)\n",
      "('episode:', 6770, '  reward:', -151)\n",
      "('episode:', 6780, '  reward:', -151)\n",
      "('episode:', 6790, '  reward:', -151)\n",
      "('episode:', 6800, '  reward:', -151)\n",
      "('episode:', 6810, '  reward:', -153)\n",
      "('episode:', 6820, '  reward:', -151)\n",
      "('episode:', 6830, '  reward:', -152)\n",
      "('episode:', 6840, '  reward:', -153)\n",
      "('episode:', 6850, '  reward:', -154)\n",
      "('episode:', 6860, '  reward:', -154)\n",
      "('episode:', 6870, '  reward:', -152)\n",
      "('episode:', 6880, '  reward:', -153)\n",
      "('episode:', 6890, '  reward:', -152)\n",
      "('episode:', 6900, '  reward:', -153)\n",
      "('episode:', 6910, '  reward:', -153)\n",
      "('episode:', 6920, '  reward:', -153)\n",
      "('episode:', 6930, '  reward:', -153)\n",
      "('episode:', 6940, '  reward:', -152)\n",
      "('episode:', 6950, '  reward:', -151)\n",
      "('episode:', 6960, '  reward:', -152)\n",
      "('episode:', 6970, '  reward:', -152)\n",
      "('episode:', 6980, '  reward:', -151)\n",
      "('episode:', 6990, '  reward:', -152)\n",
      "('episode:', 7000, '  reward:', -152)\n",
      "('episode:', 7010, '  reward:', -152)\n",
      "('episode:', 7020, '  reward:', -154)\n",
      "('episode:', 7030, '  reward:', -153)\n",
      "('episode:', 7040, '  reward:', -153)\n",
      "('episode:', 7050, '  reward:', -152)\n",
      "('episode:', 7060, '  reward:', -151)\n",
      "('episode:', 7070, '  reward:', -150)\n",
      "('episode:', 7080, '  reward:', -151)\n",
      "('episode:', 7090, '  reward:', -150)\n",
      "('episode:', 7100, '  reward:', -151)\n",
      "('episode:', 7110, '  reward:', -152)\n",
      "('episode:', 7120, '  reward:', -153)\n",
      "('episode:', 7130, '  reward:', -154)\n",
      "('episode:', 7140, '  reward:', -154)\n",
      "('episode:', 7150, '  reward:', -154)\n",
      "('episode:', 7160, '  reward:', -153)\n",
      "('episode:', 7170, '  reward:', -152)\n",
      "('episode:', 7180, '  reward:', -153)\n",
      "('episode:', 7190, '  reward:', -153)\n",
      "('episode:', 7200, '  reward:', -152)\n",
      "('episode:', 7210, '  reward:', -151)\n",
      "('episode:', 7220, '  reward:', -151)\n",
      "('episode:', 7230, '  reward:', -151)\n",
      "('episode:', 7240, '  reward:', -151)\n",
      "('episode:', 7250, '  reward:', -151)\n",
      "('episode:', 7260, '  reward:', -152)\n",
      "('episode:', 7270, '  reward:', -152)\n",
      "('episode:', 7280, '  reward:', -151)\n",
      "('episode:', 7290, '  reward:', -152)\n",
      "('episode:', 7300, '  reward:', -151)\n",
      "('episode:', 7310, '  reward:', -153)\n",
      "('episode:', 7320, '  reward:', -153)\n",
      "('episode:', 7330, '  reward:', -155)\n",
      "('episode:', 7340, '  reward:', -154)\n",
      "('episode:', 7350, '  reward:', -152)\n",
      "('episode:', 7360, '  reward:', -152)\n",
      "('episode:', 7370, '  reward:', -151)\n",
      "('episode:', 7380, '  reward:', -151)\n",
      "('episode:', 7390, '  reward:', -151)\n",
      "('episode:', 7400, '  reward:', -152)\n",
      "('episode:', 7410, '  reward:', -153)\n",
      "('episode:', 7420, '  reward:', -153)\n",
      "('episode:', 7430, '  reward:', -150)\n",
      "('episode:', 7440, '  reward:', -150)\n",
      "('episode:', 7450, '  reward:', -149)\n",
      "('episode:', 7460, '  reward:', -152)\n",
      "('episode:', 7470, '  reward:', -152)\n",
      "('episode:', 7480, '  reward:', -152)\n",
      "('episode:', 7490, '  reward:', -152)\n",
      "('episode:', 7500, '  reward:', -150)\n",
      "('episode:', 7510, '  reward:', -152)\n",
      "('episode:', 7520, '  reward:', -151)\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "MEMORY_SIZE = 10500\n",
    "BATCH_SIZE = 5000\n",
    "TRAIN_ITR = 2\n",
    "EPS_DECAY_RATE = 0.999\n",
    "actor = Actor(sess, n_features=N_F, n_actions=N_A, batch_size=BATCH_SIZE\n",
    "              , train_itr=TRAIN_ITR, eps_decay_rate=EPS_DECAY_RATE, lr=LR_A)\n",
    "critic = Critic(sess, n_features=N_F, \n",
    "                batch_size=BATCH_SIZE, train_itr=TRAIN_ITR, lr=LR_C)\n",
    "memory = Memory(MEMORY_SIZE, N_F, N_A)\n",
    "# replay = ExpReplay(MEMORY_SIZE, N_F, N_A, BATCH_SIZE)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "if OUTPUT_GRAPH:\n",
    "    tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "m = 0\n",
    "for i_episode in range(0, MAX_EPISODE):\n",
    "    s = env.reset()\n",
    "    t = 0\n",
    "    track_r = []\n",
    "    while True:\n",
    "        a = actor.choose_action(s)\n",
    "        s_, r, done, info = env.step(action_map[a, :])\n",
    "        track_r.append(r)\n",
    "        \n",
    "        if m < 10000: memory.fifo(s, a, r, s_)\n",
    "        s = s_\n",
    "        t += 1\n",
    "        m += 1\n",
    "        if t >= MAX_EP_STEPS: #done or\n",
    "            ep_rs_sum = sum(track_r)\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.95 + ep_rs_sum * 0.05\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering\n",
    "            if i_episode % 10 ==0: print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "            break\n",
    "        \n",
    "    if m >= 10000:\n",
    "        states, actions, rewards, next_states = memory.sampling()\n",
    "        mx_actions = np.zeros((actions.shape[0], N_A))\n",
    "        for i in range(0, states.shape[0]):\n",
    "            mx_actions[i, int(actions[i, 0])] = 1\n",
    "        probs = actor.get_probs(states, mx_actions)\n",
    "        td_errors = critic.get_error(states, rewards, next_states)\n",
    "        actor.learn(states, mx_actions, td_errors)\n",
    "        critic.learn(states, rewards, next_states, probs)\n",
    "        actor.update_policy()\n",
    "        memory.empty_memory()\n",
    "        m = 0     \n",
    "    if i_episode % 5000 == 0:\n",
    "        saver.save(sess, \"./reacher_model/train.ckpt\", global_step=i_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "m = 0\n",
    "for i_episode in range(100000, 2*MAX_EPISODE):\n",
    "    s = env.reset()\n",
    "    t = 0\n",
    "    track_r = []\n",
    "    while True:\n",
    "        a = actor.choose_action(s)\n",
    "        s_, r, done, info = env.step(action_map[a, :])\n",
    "        track_r.append(r)\n",
    "        \n",
    "        if m < 10000: memory.fifo(s, a, r, s_)\n",
    "        s = s_\n",
    "        t += 1\n",
    "        m += 1\n",
    "        if t >= MAX_EP_STEPS: #done or\n",
    "            ep_rs_sum = sum(track_r)\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.95 + ep_rs_sum * 0.05\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering\n",
    "            if i_episode % 10 ==0: print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "            break\n",
    "        \n",
    "    if m >= 10000:\n",
    "        states, actions, rewards, next_states = memory.sampling()\n",
    "        mx_actions = np.zeros((actions.shape[0], N_A))\n",
    "        for i in range(0, states.shape[0]):\n",
    "            mx_actions[i, int(actions[i, 0])] = 1\n",
    "        probs = actor.get_probs(states, mx_actions)\n",
    "        td_errors = critic.get_error(states, rewards, next_states)\n",
    "        actor.learn(states, mx_actions, td_errors)\n",
    "        critic.learn(states, rewards, next_states, probs)\n",
    "        actor.update_policy()\n",
    "        memory.empty_memory()\n",
    "        m = 0     \n",
    "    if i_episode % 5000 == 0:\n",
    "        saver.save(sess, \"./reacher_model/train.ckpt\", global_step=i_episode)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "m = 0\n",
    "for i_episode in range(2*MAX_EPISODE, 3*MAX_EPISODE):\n",
    "    s = env.reset()\n",
    "    t = 0\n",
    "    track_r = []\n",
    "    while True:\n",
    "        a = actor.choose_action(s)\n",
    "        s_, r, done, info = env.step(action_map[a, :])\n",
    "        track_r.append(r)\n",
    "        \n",
    "        if m < 10000: memory.fifo(s, a, r, s_)\n",
    "        s = s_\n",
    "        t += 1\n",
    "        m += 1\n",
    "        if t >= MAX_EP_STEPS: #done or\n",
    "            ep_rs_sum = sum(track_r)\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.95 + ep_rs_sum * 0.05\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering\n",
    "            if i_episode % 10 ==0: print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "            break\n",
    "        \n",
    "    if m >= 10000:\n",
    "        states, actions, rewards, next_states = memory.sampling()\n",
    "        mx_actions = np.zeros((actions.shape[0], N_A))\n",
    "        for i in range(0, states.shape[0]):\n",
    "            mx_actions[i, int(actions[i, 0])] = 1\n",
    "        probs = actor.get_probs(states, mx_actions)\n",
    "        td_errors = critic.get_error(states, rewards, next_states)\n",
    "        actor.learn(states, mx_actions, td_errors)\n",
    "        critic.learn(states, rewards, next_states, probs)\n",
    "        actor.update_policy()\n",
    "        memory.empty_memory()\n",
    "        m = 0     \n",
    "    if i_episode % 5000 == 0:\n",
    "        saver.save(sess, \"./reacher_model/train.ckpt\", global_step=i_episode)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(0, 30):\n",
    "    s = env.reset()\n",
    "    t = 0\n",
    "    track_r = []\n",
    "    while True:\n",
    "        env.render()\n",
    "        a = actor.choose_action(s)\n",
    "        s_, r, done, info = env.step(action_map[a, :])\n",
    "        track_r.append(r)\n",
    "        s = s_\n",
    "        t += 1\n",
    "        if t >= MAX_EP_STEPS: #done or\n",
    "            ep_rs_sum = sum(track_r)\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.95 + ep_rs_sum * 0.05\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering\n",
    "            print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
