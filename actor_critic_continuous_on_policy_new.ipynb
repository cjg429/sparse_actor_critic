{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(object):\n",
    "    def __init__(self, obs_dim, act_dim, clip_range=0.2,\n",
    "                 epochs=10, lr=3e-5, hdim=64, mdn_weight=\"sparsemax\", n_mixture=4, max_std=1.0,\n",
    "                 seed=0,alpha=1.0):\n",
    "        self.alpha=alpha\n",
    "        self.seed=seed\n",
    "        \n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        \n",
    "        self.clip_range = clip_range\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.hdim = hdim\n",
    "        self.mdn_weight = mdn_weight\n",
    "        self.n_mixture = n_mixture\n",
    "        self.max_std = max_std\n",
    "        \n",
    "        self._build_graph()\n",
    "        self._init_session()\n",
    "\n",
    "    def _build_graph(self):\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            self._placeholders()\n",
    "            self._policy_nn()\n",
    "            self._logprob()\n",
    "            self._kl_entropy()\n",
    "            self._loss_train_op()\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            self.variables = tf.global_variables()\n",
    "            \n",
    "    def _placeholders(self):\n",
    "        # observations, actions and advantages:\n",
    "        self.obs_ph = tf.placeholder(tf.float32, (None, self.obs_dim), 'obs')\n",
    "        self.act_ph = tf.placeholder(tf.float32, (None, self.act_dim), 'act')\n",
    "        self.advantages_ph = tf.placeholder(tf.float32, (None,), 'advantages')\n",
    "\n",
    "        # learning rate:\n",
    "        self.lr_ph = tf.placeholder(tf.float32, (), 'lr')\n",
    "        \n",
    "        # place holder for old parameters\n",
    "        self.old_std_ph = tf.placeholder(tf.float32, (None, self.act_dim, self.n_mixture), 'old_std')\n",
    "        self.old_mean_ph = tf.placeholder(tf.float32, (None, self.act_dim, self.n_mixture), 'old_means')\n",
    "        self.old_pi_ph = tf.placeholder(tf.float32, (None, self.n_mixture), 'old_pi')\n",
    "\n",
    "    def _policy_nn(self):\n",
    "        \n",
    "        hid1_size = self.hdim\n",
    "        hid2_size = self.hdim\n",
    "        \n",
    "        # TWO HIDDEN LAYERS\n",
    "        out = tf.layers.dense(self.obs_ph, hid1_size, tf.tanh,\n",
    "                              kernel_initializer=tf.random_normal_initializer(stddev=0.01,seed= self.seed), name=\"h1\")\n",
    "        out = tf.layers.dense(out, hid2_size, tf.tanh,\n",
    "                              kernel_initializer=tf.random_normal_initializer(stddev=0.01,seed= self.seed), name=\"h2\")\n",
    "        means = tf.layers.dense(out, self.act_dim*self.n_mixture,\n",
    "                                kernel_initializer=tf.random_normal_initializer(stddev=0.01,seed= self.seed), \n",
    "                                name=\"flat_mean\")\n",
    "        self.mean = tf.reshape(means,shape=[-1,self.act_dim,self.n_mixture], name=\"mean\")\n",
    "        logits_std = tf.layers.dense(out, self.act_dim*self.n_mixture,\n",
    "                                kernel_initializer=tf.random_normal_initializer(stddev=0.01,seed= self.seed), \n",
    "                                name=\"flat_logits_std\")\n",
    "        self.std = tf.reshape(self.max_std*tf.sigmoid(logits_std),shape=[-1,self.act_dim,self.n_mixture], name=\"std\")\n",
    "        if self.mdn_weight==\"softmax\":\n",
    "            self.pi = tf.nn.softmax(tf.layers.dense(out, self.n_mixture,\n",
    "                                                kernel_initializer=tf.random_normal_initializer(stddev=0.01,seed= self.seed), name=\"pi\"))\n",
    "        elif self.mdn_weight==\"sparsemax\":\n",
    "            self.pi = tf.contrib.sparsemax.sparsemax(tf.layers.dense(out, self.n_mixture,\n",
    "                                                kernel_initializer=tf.random_normal_initializer(stddev=0.01,seed= self.seed), name=\"pi\"))\n",
    "        \n",
    "    def _logprob(self):\n",
    "        # PROBABILITY WITH TRAINING PARAMETER\n",
    "        y = self.act_ph \n",
    "        mu = self.mean\n",
    "        sigma = self.std\n",
    "        pi = self.pi\n",
    "        \n",
    "        quadratics = -0.5*tf.reduce_sum(tf.square((tf.tile(y[:,:,tf.newaxis],[1,1,self.n_mixture])-mu)/sigma),axis=1)\n",
    "        logdet = -0.5*tf.reduce_sum(tf.log(sigma),axis=1)\n",
    "        logconstant = - 0.5*self.act_dim*np.log(2.*np.pi)\n",
    "        logpi = tf.log(pi + 1e-8)\n",
    "        \n",
    "        exponents = quadratics + logdet + logconstant + logpi\n",
    "        logprobs = tf.reduce_logsumexp(exponents,axis=1)\n",
    "        \n",
    "        self.logp = logprobs\n",
    "\n",
    "        old_mu_ph = self.old_mean_ph\n",
    "        old_sigma_ph = self.old_std_ph\n",
    "        old_pi_ph = self.old_pi_ph\n",
    "    \n",
    "        quadratics = -0.5*tf.reduce_sum(tf.square((tf.tile(y[:,:,tf.newaxis],[1,1,self.n_mixture])-old_mu_ph)/old_sigma_ph),axis=1)\n",
    "        logdet = -0.5*tf.reduce_sum(tf.log(old_sigma_ph),axis=1)\n",
    "        logconstant = - 0.5*self.act_dim*np.log(2.*np.pi)\n",
    "        logpi = tf.log(old_pi_ph + 1e-8)\n",
    "        \n",
    "        exponents = quadratics + logdet + logconstant + logpi\n",
    "        old_logprobs = tf.reduce_logsumexp(exponents,axis=1)\n",
    "        \n",
    "        self.logp_old = old_logprobs\n",
    "        \n",
    "    def _kl_entropy(self):\n",
    "        \n",
    "        def energy(mu1,std1,pi1,mu2,std2,pi2):\n",
    "            energy_components = []\n",
    "            for i in range(self.n_mixture):\n",
    "                for j in range(self.n_mixture):\n",
    "                    mu1i = mu1[:,:,i] \n",
    "                    mu2j = mu2[:,:,j]\n",
    "                    std1i = std1[:,:,i]\n",
    "                    std2j = std2[:,:,j]\n",
    "                    pi1i = pi1[:,i]\n",
    "                    pi2j = pi2[:,j]\n",
    "                    energy_components.append(pi1i*pi2j * tf.exp(-0.5*tf.reduce_sum(((mu1i - mu2j)/(std1i+std2j))**2+2.*tf.log(std1i+std2j)+np.log(2*np.pi),axis=1)))\n",
    "            return tf.reduce_sum(tf.stack(energy_components,axis=1),axis=1) \n",
    "            \n",
    "        mean, std, weight = self.mean, self.std, self.pi\n",
    "        old_mean, old_std, old_weight = self.old_mean_ph, self.old_std_ph, self.old_pi_ph\n",
    "\n",
    "#         weight = weight/tf.reduce_sum(weight,axis=1,keep_dims=True)\n",
    "#         old_weight = old_weight/tf.reduce_sum(old_weight,axis=1,keep_dims=True)\n",
    "        \n",
    "        if self.mdn_weight==\"softmax\":\n",
    "            self.entropy = tf.reduce_sum(self.pi*(-tf.log(self.pi) + 0.5 * (self.act_dim * (np.log(2 * np.pi) + 1) +\n",
    "                                                                        tf.reduce_sum(tf.log(std),axis=1))),axis=1)\n",
    "            self.entropy = tf.reduce_mean(self.entropy)\n",
    "        elif self.mdn_weight==\"sparsemax\":\n",
    "            self.entropy = tf.reduce_mean(0.5*(1-energy(mean, std, weight,mean, std, weight)))\n",
    "            \n",
    "        log_det_cov_old = tf.reduce_sum(tf.log(old_std),axis=1)\n",
    "        log_det_cov_new = tf.reduce_sum(tf.log(std),axis=1)\n",
    "        tr_old_new = tf.reduce_sum(old_std/std,axis=1)\n",
    "\n",
    "        kl = tf.reduce_sum(old_weight*tf.log((old_weight+1e-8)/(weight+1e-8)) + 0.5 * old_weight*(log_det_cov_new - log_det_cov_old + tr_old_new +\n",
    "                         tf.reduce_sum(tf.square((mean - old_mean)/std),axis=1) - self.act_dim),axis=1)\n",
    "        self.kl = tf.reduce_mean(kl)\n",
    "        \n",
    "    def _loss_train_op(self):\n",
    "        \n",
    "        # Proximal Policy Optimization CLIPPED LOSS FUNCTION\n",
    "#         ratio = tf.exp(self.logp - self.logp_old) \n",
    "#         clipped_ratio = tf.clip_by_value(ratio,clip_value_min=1-self.clip_range,clip_value_max=1+self.clip_range) \n",
    "#         self.loss = -tf.reduce_mean(tf.minimum(self.advantages_ph*ratio,self.advantages_ph*clipped_ratio))\n",
    "                \n",
    "        def energy(mu1,std1,pi1,mu2,std2,pi2):\n",
    "            energy_components = []\n",
    "            for i in range(self.n_mixture):\n",
    "                for j in range(self.n_mixture):\n",
    "                    mu1i = mu1[:,:,i] \n",
    "                    mu2j = mu2[:,:,j]\n",
    "                    std1i = std1[:,:,i]\n",
    "                    std2j = std2[:,:,j]\n",
    "                    pi1i = pi1[:,i]\n",
    "                    pi2j = pi2[:,j]\n",
    "                    energy_components.append(pi1i*pi2j * tf.exp(-0.5*tf.reduce_sum(((mu1i - mu2j)/(std1i+std2j))**2+2.*tf.log(std1i+std2j)+np.log(2*np.pi),axis=1)))\n",
    "            return tf.reduce_sum(tf.stack(energy_components,axis=1),axis=1) \n",
    "            \n",
    "        mean, std, weight = self.mean, self.std, self.pi\n",
    "        \n",
    "        alpha = self.alpha\n",
    "        self.error = tf.maximum(self.advantages_ph + alpha*(0.5 + 0.5*energy(mean, std, weight, mean, std, weight)), \n",
    "                                0)- alpha*tf.exp(self.logp)\n",
    "        self.loss = tf.reduce_mean(tf.square(self.error))\n",
    "        # OPTIMIZER \n",
    "        optimizer = tf.train.AdamOptimizer(self.lr_ph)\n",
    "        self.train_op = optimizer.minimize(self.loss)\n",
    "\n",
    "    def _init_session(self):\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=config,graph=self.g)\n",
    "        self.sess.run(self.init)\n",
    "\n",
    "    def sample(self, obs): # SAMPLE FROM POLICY\n",
    "        feed_dict = {self.obs_ph: obs}\n",
    "        pi, mu, sigma = self.sess.run([self.pi, self.mean, self.std],feed_dict=feed_dict)\n",
    "        pi = (pi+1e-8)/np.sum(pi+1e-8,axis=1,keepdims=True)\n",
    "        sigma = sigma\n",
    "        n_points = np.shape(obs)[0]\n",
    "        \n",
    "        _y_sampled = np.zeros([n_points,self.act_dim])\n",
    "        for i in range(n_points):\n",
    "            k = np.random.choice(self.n_mixture,p=pi[i,:])\n",
    "            _y_sampled[i,:] = mu[i,:,k] + np.random.randn(1,self.act_dim)*sigma[i,:,k]\n",
    "        return _y_sampled\n",
    "        \n",
    "    def control(self, obs): # COMPUTE MEAN\n",
    "        feed_dict = {self.obs_ph: obs}\n",
    "        pi, mu, sigma = self.sess.run([self.pi, self.mean, self.std],feed_dict=feed_dict)\n",
    "        pi = (pi+1e-8)/np.sum(pi+1e-8,axis=1,keepdims=True)\n",
    "        sigma = sigma\n",
    "        n_points = np.shape(obs)[0]\n",
    "        \n",
    "        _y_sampled = np.zeros([n_points,self.act_dim])\n",
    "        for i in range(n_points):\n",
    "            k = np.argmax(pi[i,:])\n",
    "            _y_sampled[i,:] = mu[i,:,k] + np.random.randn(1,self.act_dim)*sigma[i,:,k]\n",
    "        return _y_sampled        \n",
    "    \n",
    "    def update(self, observes, actions, advantages, batch_size = 128): # TRAIN POLICY\n",
    "        \n",
    "        num_batches = max(observes.shape[0] // batch_size, 1)\n",
    "        batch_size = observes.shape[0] // num_batches\n",
    "        \n",
    "        old_means_np, old_std_np, old_pi_np = self.sess.run([self.mean, self.std, self.pi],{self.obs_ph: observes}) # COMPUTE OLD PARAMTER\n",
    "        for e in range(self.epochs):\n",
    "            observes, actions, advantages, old_means_np, old_std_np = shuffle(observes, actions, advantages, old_means_np, old_std_np, random_state=self.seed)\n",
    "            for j in range(num_batches): \n",
    "                start = j * batch_size\n",
    "                end = (j + 1) * batch_size\n",
    "                feed_dict = {self.obs_ph: observes[start:end,:],\n",
    "                     self.act_ph: actions[start:end,:],\n",
    "                     self.advantages_ph: advantages[start:end],\n",
    "                     self.old_std_ph: old_std_np[start:end,:,:],\n",
    "                     self.old_mean_ph: old_means_np[start:end,:,:],\n",
    "                     self.old_pi_ph: old_pi_np[start:end,:],\n",
    "                     self.lr_ph: self.lr}        \n",
    "                self.sess.run(self.train_op, feed_dict)\n",
    "            \n",
    "        feed_dict = {self.obs_ph: observes,\n",
    "                 self.act_ph: actions,\n",
    "                 self.advantages_ph: advantages,\n",
    "                 self.old_std_ph: old_std_np,\n",
    "                 self.old_mean_ph: old_means_np,\n",
    "                 self.old_pi_ph: old_pi_np,\n",
    "                 self.lr_ph: self.lr}             \n",
    "        loss, kl, entropy = self.sess.run([self.loss, self.kl, self.entropy], feed_dict)\n",
    "        return loss, kl, entropy\n",
    "    \n",
    "    def close_sess(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(object):\n",
    "    def __init__(self, obs_dim, epochs=20, lr=1e-4, hdim=64, seed=0):\n",
    "        self.seed = seed\n",
    "    \n",
    "        self.obs_dim = obs_dim\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.hdim = hdim\n",
    "        \n",
    "        self._build_graph()\n",
    "        self._init_session()\n",
    "        \n",
    "    def _build_graph(self):\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            self.obs_ph = tf.placeholder(tf.float32, (None, self.obs_dim), 'obs_valfunc')\n",
    "            self.val_ph = tf.placeholder(tf.float32, (None,), 'val_valfunc')\n",
    "            \n",
    "            hid1_size = self.hdim \n",
    "            hid2_size = self.hdim \n",
    "            \n",
    "            out = tf.layers.dense(self.obs_ph, hid1_size, tf.tanh,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(\n",
    "                                      stddev=0.01,seed=self.seed), name=\"h1\")\n",
    "            out = tf.layers.dense(out, hid2_size, tf.tanh,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(\n",
    "                                      stddev=0.01,seed=self.seed), name=\"h2\")\n",
    "            out = tf.layers.dense(out, 1,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(\n",
    "                                      stddev=0.01,seed=self.seed), name='output')\n",
    "            self.out = tf.squeeze(out)\n",
    "            \n",
    "            # L2 LOSS\n",
    "            self.loss = tf.reduce_mean(tf.square(self.out - self.val_ph))\n",
    "            \n",
    "            # OPTIMIZER\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "            self.train_op = optimizer.minimize(self.loss)\n",
    "            \n",
    "            self.init = tf.global_variables_initializer()\n",
    "            self.variables = tf.global_variables()\n",
    "    \n",
    "    def _init_session(self):\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=config,graph=self.g)\n",
    "        self.sess.run(self.init)\n",
    "\n",
    "    def fit(self, x, y, batch_size=32):\n",
    "        num_batches = max(x.shape[0] // batch_size, 1)\n",
    "        x_train, y_train = x, y\n",
    "        for e in range(self.epochs):\n",
    "            x_train, y_train = shuffle(x_train, y_train, random_state=self.seed)\n",
    "            for j in range(num_batches):\n",
    "                start = j * batch_size\n",
    "                end = (j + 1) * batch_size\n",
    "                feed_dict = {self.obs_ph: x_train[start:end, :],\n",
    "                             self.val_ph: y_train[start:end]}\n",
    "                self.sess.run([self.train_op], feed_dict=feed_dict)\n",
    "        feed_dict = {self.obs_ph: x_train,\n",
    "                     self.val_ph: y_train}\n",
    "        loss, = self.sess.run([self.loss], feed_dict=feed_dict)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, x): # PREDICT VALUE OF THE GIVEN STATE\n",
    "        feed_dict = {self.obs_ph: x}\n",
    "        y_hat = self.sess.run(self.out, feed_dict=feed_dict)\n",
    "        return np.squeeze(y_hat)\n",
    "\n",
    "    def close_sess(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount(x, gamma=0.99): # compute discount\n",
    "    return scipy.signal.lfilter([1.0], [1.0, -gamma], x[::-1])[::-1]\n",
    "\n",
    "def add_value(trajectories, val_func): # Add value estimation for each trajectories\n",
    "    for trajectory in trajectories:\n",
    "        observes = trajectory['observes']\n",
    "        values = val_func.predict(observes)\n",
    "        trajectory['values'] = values\n",
    "\n",
    "def add_gae(trajectories, gamma=0.99, lam=0.98): # generalized advantage estimation (for training stability)\n",
    "    for trajectory in trajectories:\n",
    "        rewards = trajectory['rewards']\n",
    "        values = trajectory['values']\n",
    "        \n",
    "        # temporal differences\n",
    "        tds = rewards - values + np.append(values[1:] * gamma, 0)\n",
    "        advantages = discount(tds, gamma * lam)\n",
    "        \n",
    "        trajectory['advantages'] = advantages\n",
    "        trajectory['returns'] = values+advantages\n",
    "\n",
    "def build_train_set(trajectories):\n",
    "    observes = np.concatenate([t['observes'] for t in trajectories])\n",
    "    actions = np.concatenate([t['actions'] for t in trajectories])\n",
    "    returns = np.concatenate([t['returns'] for t in trajectories])\n",
    "    advantages = np.concatenate([t['advantages'] for t in trajectories])\n",
    "\n",
    "    # Normalization of advantages \n",
    "    # In baselines, which is a github repo including implementation of PPO by OpenAI, \n",
    "    # all policy gradient methods use advantage normalization trick as belows.\n",
    "    # The insight under this trick is that it tries to move policy parameter towards locally maximum point.\n",
    "    # Sometimes, this trick doesnot work.\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-6)\n",
    "\n",
    "    return observes, actions, advantages, returns\n",
    "\n",
    "def run_episode(env, policy, animate=False, evaluation=False): # Run policy and collect (state, action, reward) pairs\n",
    "    obs = env.reset()\n",
    "    observes, actions, rewards, infos = [], [], [], []\n",
    "    done = False\n",
    "    while not done:\n",
    "        if animate:\n",
    "            env.render()\n",
    "        obs = obs.astype(np.float32).reshape((1, -1))\n",
    "        observes.append(obs)\n",
    "        if evaluation:\n",
    "            action = policy.control(obs).reshape((1, -1)).astype(np.float32)\n",
    "        else:\n",
    "            action = policy.sample(obs).reshape((1, -1)).astype(np.float32)\n",
    "        actions.append(action)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        if not isinstance(reward, float):\n",
    "            reward = np.asscalar(reward)\n",
    "        rewards.append(reward)\n",
    "        infos.append(info)\n",
    "        \n",
    "    return (np.concatenate(observes), np.concatenate(actions), np.array(rewards, dtype=np.float32), infos)\n",
    "\n",
    "def run_policy(env, policy, episodes, evaluation=False): # collect trajectories. if 'evaluation' is ture, then only mean value of policy distribution is used without sampling.\n",
    "    total_steps = 0\n",
    "    trajectories = []\n",
    "    for e in range(episodes):\n",
    "        observes, actions, rewards, infos = run_episode(env, policy, evaluation=evaluation)\n",
    "        total_steps += observes.shape[0]\n",
    "        trajectory = {'observes': observes,\n",
    "                      'actions': actions,\n",
    "                      'rewards': rewards,\n",
    "                      'infos': infos}\n",
    "        trajectories.append(trajectory)\n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-06 16:11:56,968] Making new env: Pendulum-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100] Mean Ret : -1225.243, Value Loss : 2520.135, Policy loss : 0.91378, Policy KL : 38181.12500, Policy Entropy : 0.229 ***\n",
      "[5/100] Mean Ret : -1003.503, Value Loss : 5033.092, Policy loss : 1.11494, Policy KL : 385161493479424.00000, Policy Entropy : 0.402 ***\n",
      "[10/100] Mean Ret : -903.130, Value Loss : 4928.692, Policy loss : 1.11335, Policy KL : 26140194816.00000, Policy Entropy : 0.400 ***\n",
      "[15/100] Mean Ret : -854.486, Value Loss : 4656.898, Policy loss : 1.11338, Policy KL : 82307383296.00000, Policy Entropy : 0.400 ***\n",
      "[20/100] Mean Ret : -783.526, Value Loss : 4081.543, Policy loss : 1.11193, Policy KL : 8950589440.00000, Policy Entropy : 0.400 ***\n",
      "[25/100] Mean Ret : -305.748, Value Loss : 1130.750, Policy loss : 1.09559, Policy KL : 664681408.00000, Policy Entropy : 0.401 ***\n",
      "[30/100] Mean Ret : -221.270, Value Loss : 257.654, Policy loss : 0.49526, Policy KL : 283.76273, Policy Entropy : 0.264 ***\n",
      "[35/100] Mean Ret : -149.261, Value Loss : 3.299, Policy loss : 1.10694, Policy KL : 24691686.00000, Policy Entropy : 0.400 ***\n",
      "[40/100] Mean Ret : -149.088, Value Loss : 0.366, Policy loss : 1.10178, Policy KL : 26942805901312.00000, Policy Entropy : 0.399 ***\n",
      "[45/100] Mean Ret : -151.019, Value Loss : 1.777, Policy loss : 0.96655, Policy KL : 622633943040.00000, Policy Entropy : 0.381 ***\n",
      "[50/100] Mean Ret : -152.832, Value Loss : 23.680, Policy loss : 0.86603, Policy KL : 15440915988480.00000, Policy Entropy : 0.376 ***\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b902bd942c1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mobserves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madvantages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_train_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mpol_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpol_kl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpol_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobserves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madvantages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mvf_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobserves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-20726f7636a9>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, observes, actions, advantages, batch_size)\u001b[0m\n\u001b[1;32m    219\u001b[0m                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mold_pi_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mold_pi_np\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                      self.lr_ph: self.lr}        \n\u001b[0;32m--> 221\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         feed_dict = {self.obs_ph: observes,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "\n",
    "env.seed(seed=seed)\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "\n",
    "policy = Policy(obs_dim, act_dim, epochs=30, hdim=32, lr=3e-4, clip_range=0.2,seed=seed)\n",
    "val_func = Value(obs_dim, epochs=50, hdim=32, lr=1e-3, seed=seed)\n",
    "\n",
    "episode_size = 100\n",
    "batch_size = 64\n",
    "nupdates = 100\n",
    "\n",
    "for update in range(nupdates+1):\n",
    "\n",
    "    trajectories = run_policy(env, policy, episodes=episode_size)\n",
    "\n",
    "    add_value(trajectories, val_func)\n",
    "    add_gae(trajectories)\n",
    "    observes, actions, advantages, returns = build_train_set(trajectories)\n",
    "\n",
    "    pol_loss, pol_kl, pol_entropy = policy.update(observes, actions, advantages, batch_size=batch_size)  \n",
    "    vf_loss = val_func.fit(observes, returns,batch_size=batch_size)\n",
    "    \n",
    "    mean_ret = np.mean([np.sum(t['rewards']) for t in trajectories])\n",
    "    if (update%5) == 0:\n",
    "        print('[{}/{}] Mean Ret : {:.3f}, Value Loss : {:.3f}, Policy loss : {:.5f}, Policy KL : {:.5f}, Policy Entropy : {:.3f} ***'.format(\n",
    "                            update, nupdates, mean_ret, vf_loss, pol_loss, pol_kl, pol_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = run_policy(env, policy, episodes=100, evaluation=True)\n",
    "mean_ret = np.mean([np.sum(t['rewards']) for t in trajectories])\n",
    "print('Results: {}'.format(mean_ret))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
