{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import maxapproxi\n",
    "\n",
    "np.random.seed(2)\n",
    "tf.set_random_seed(2)  # reproducible\n",
    "\n",
    "# Superparameters\n",
    "OUTPUT_GRAPH = False\n",
    "MAX_EPISODE = 3000\n",
    "DISPLAY_REWARD_THRESHOLD = 200  # renders environment if total episode reward is greater then this threshold\n",
    "MAX_EP_STEPS = 1000   # maximum time step in one episode\n",
    "RENDER = False  # rendering wastes time\n",
    "GAMMA = 0.9     # reward discount in TD error\n",
    "ALPHA = 1.0 \n",
    "LR_A = 0.001    # learning rate for actor\n",
    "LR_C = 0.01     # learning rate for critic\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(1)  # reproducible\n",
    "env = env.unwrapped\n",
    "\n",
    "N_F = env.observation_space.shape[0]\n",
    "N_A = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpReplay(object):\n",
    "    def __init__(self, memory_size, state_dim, act_dim, batch_size):\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.state_dim = state_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.states = np.empty((self.memory_size, self.state_dim), dtype=np.float32)\n",
    "        self.actions = np.empty((self.memory_size, 1), dtype=np.float32)\n",
    "        self.rewards = np.empty(self.memory_size, dtype=np.float32)\n",
    "        self.next_states = np.empty((self.memory_size, self.state_dim), dtype=np.float32)\n",
    "        self.count = 0\n",
    "        self.current = 0\n",
    "    def fifo(self, state, action, reward, next_state):\n",
    "        self.states[self.current] = state\n",
    "        self.actions[self.current] = action\n",
    "        self.rewards[self.current] = reward\n",
    "        self.next_states[self.current] = next_state\n",
    "        self.current = (self.current + 1) % self.memory_size\n",
    "        self.count = self.count + 1\n",
    "    def add_trajectory(self, states, actions, rewards, next_states):\n",
    "        num = len(observes)\n",
    "        for i in range(0, num):\n",
    "            self.fifo(states[i], actions[i], rewards[i], next_states[i])\n",
    "    def sampling(self):\n",
    "        indexes = np.random.randint(min(self.count, self.memory_size), size=self.batch_size)\n",
    "        states = self.states[indexes]\n",
    "        actions = self.actions[indexes]\n",
    "        rewards = self.rewards[indexes]\n",
    "        next_states = self.next_states[indexes]\n",
    "        states = states.reshape((-1, self.state_dim))\n",
    "        actions = actions.reshape((-1, 1))\n",
    "        rewards = rewards.reshape((-1, 1))\n",
    "        next_states = next_states.reshape((-1, self.state_dim))\n",
    "        return states, actions, rewards, next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(object):\n",
    "    def __init__(self, sess, n_features, n_actions, lr=0.001):\n",
    "        self.sess = sess\n",
    "\n",
    "        self.s = tf.placeholder(tf.float32, (None, n_features), \"state\")\n",
    "        self.a = tf.placeholder(tf.float32, (None, n_actions), \"act\")\n",
    "        self.td_error = tf.placeholder(tf.float32, (None, 1), \"td_error\")  # TD_error]\n",
    "        self.eps = 1\n",
    "        self.eps_decay_rate = 0.0\n",
    "        self.eps_min = 0\n",
    "\n",
    "        with tf.variable_scope('Actor'):\n",
    "            l1 = tf.layers.dense(\n",
    "                inputs=self.s,\n",
    "                units=20,    # number of hidden units\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),    # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l1'\n",
    "            )\n",
    "\n",
    "            self.acts_prob = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=n_actions,    # output units\n",
    "                activation=tf.contrib.sparsemax.sparsemax,   # get action probabilities\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='acts_prob'\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope('exp_v'):\n",
    "            self.error = self.td_error + ALPHA*0.5*tf.reduce_sum(\n",
    "                tf.multiply(self.acts_prob, self.acts_prob), 1, keep_dims=True) + ALPHA*0.5 - ALPHA*tf.reduce_sum(\n",
    "                tf.multiply(self.acts_prob, self.a), 1, keep_dims = True)\n",
    "            # TD error + ALPHA(0.5*pi^2 + 0.5 - pi)\n",
    "            self.loss = tf.reduce_sum(tf.square(self.error))\n",
    "\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)  \n",
    "\n",
    "    def learn(self, s, a, td):\n",
    "        feed_dict = {self.s: s, self.a: a, self.td_error: td}\n",
    "        _, loss = self.sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        s = s[np.newaxis, :]\n",
    "        probs = self.sess.run(self.acts_prob, {self.s: s})   # get probabilities for all actions\n",
    "        probs = probs.ravel()\n",
    "        probs = np.ones(probs.shape[0]) * self.eps / probs.shape[0] + probs * (1. - self.eps)\n",
    "        probs = probs/np.sum(probs)\n",
    "        return np.random.choice(np.arange(probs.shape[0]), p=probs)   # return a int\n",
    "    \n",
    "    def update_policy(self):\n",
    "        self.eps = np.max((self.eps*self.eps_decay_rate, self.eps_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(object):\n",
    "    def __init__(self, sess, n_features, lr=0.01):\n",
    "        self.sess = sess\n",
    "\n",
    "        self.s = tf.placeholder(tf.float32, (None, n_features), \"state\")\n",
    "        self.v_ = tf.placeholder(tf.float32, (None, 1), \"v_next\")\n",
    "        self.r = tf.placeholder(tf.float32, (None, 1), 'r')\n",
    "\n",
    "        with tf.variable_scope('Critic'):\n",
    "            l1 = tf.layers.dense(\n",
    "                inputs=self.s,\n",
    "                units=20,  # number of hidden units\n",
    "                activation=tf.nn.relu,  # None\n",
    "                # have to be linear to make sure the convergence of actor.\n",
    "                # But linear approximator seems hardly learns the correct Q.\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l1'\n",
    "            )\n",
    "\n",
    "            self.v = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=1,  # output units\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='V'\n",
    "            )\n",
    "            \n",
    "        with tf.variable_scope('squared_TD_error'):\n",
    "            self.td_error = self.r + GAMMA * self.v_ - self.v\n",
    "            self.loss = tf.reduce_sum(tf.square(self.td_error))    # TD_error = (r+gamma*V_next) - V_eval\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
    "\n",
    "    def learn(self, s, r, s_):\n",
    "        v_ = self.sess.run(self.v, {self.s: s_})\n",
    "        td_error, _ = self.sess.run([self.td_error, self.train_op],\n",
    "                                          {self.s: s, self.v_: v_, self.r: r})\n",
    "        return td_error\n",
    "    \n",
    "    def get_error(self, s, r, s_):\n",
    "        v_ = self.sess.run(self.v, {self.s: s_})\n",
    "        td_error = self.sess.run(self.td_error,\n",
    "                                          {self.s: s, self.v_: v_, self.r: r})\n",
    "        return td_error\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('episode:', 0, '  reward:', -7)\n",
      "('episode:', 1, '  reward:', -6)\n",
      "('episode:', 2, '  reward:', -6)\n",
      "('episode:', 3, '  reward:', -6)\n",
      "('episode:', 4, '  reward:', -6)\n",
      "('episode:', 5, '  reward:', -6)\n",
      "('episode:', 6, '  reward:', -5)\n",
      "('episode:', 7, '  reward:', -5)\n",
      "('episode:', 8, '  reward:', -5)\n",
      "('episode:', 9, '  reward:', -5)\n",
      "('episode:', 10, '  reward:', -4)\n",
      "('episode:', 11, '  reward:', -4)\n",
      "('episode:', 12, '  reward:', -5)\n",
      "('episode:', 13, '  reward:', -4)\n",
      "('episode:', 14, '  reward:', -4)\n",
      "('episode:', 15, '  reward:', -4)\n",
      "('episode:', 16, '  reward:', -4)\n",
      "('episode:', 17, '  reward:', -4)\n",
      "('episode:', 18, '  reward:', -5)\n",
      "('episode:', 19, '  reward:', -4)\n",
      "('episode:', 20, '  reward:', -3)\n",
      "('episode:', 21, '  reward:', -3)\n",
      "('episode:', 22, '  reward:', -4)\n",
      "('episode:', 23, '  reward:', -3)\n",
      "('episode:', 24, '  reward:', -3)\n",
      "('episode:', 25, '  reward:', -3)\n",
      "('episode:', 26, '  reward:', -3)\n",
      "('episode:', 27, '  reward:', -3)\n",
      "('episode:', 28, '  reward:', -3)\n",
      "('episode:', 29, '  reward:', -3)\n",
      "('episode:', 30, '  reward:', -2)\n",
      "('episode:', 31, '  reward:', -3)\n",
      "('episode:', 32, '  reward:', -3)\n",
      "('episode:', 33, '  reward:', -2)\n",
      "('episode:', 34, '  reward:', -2)\n",
      "('episode:', 35, '  reward:', -2)\n",
      "('episode:', 36, '  reward:', -2)\n",
      "('episode:', 37, '  reward:', -3)\n",
      "('episode:', 38, '  reward:', -3)\n",
      "('episode:', 39, '  reward:', -2)\n",
      "('episode:', 40, '  reward:', -2)\n",
      "('episode:', 41, '  reward:', -1)\n",
      "('episode:', 42, '  reward:', -1)\n",
      "('episode:', 43, '  reward:', -1)\n",
      "('episode:', 44, '  reward:', -1)\n",
      "('episode:', 45, '  reward:', 0)\n",
      "('episode:', 46, '  reward:', 0)\n",
      "('episode:', 47, '  reward:', 0)\n",
      "('episode:', 48, '  reward:', 0)\n",
      "('episode:', 49, '  reward:', 0)\n",
      "('episode:', 50, '  reward:', 0)\n",
      "('episode:', 51, '  reward:', 0)\n",
      "('episode:', 52, '  reward:', 1)\n",
      "('episode:', 53, '  reward:', 1)\n",
      "('episode:', 54, '  reward:', 0)\n",
      "('episode:', 55, '  reward:', 1)\n",
      "('episode:', 56, '  reward:', 1)\n",
      "('episode:', 57, '  reward:', 1)\n",
      "('episode:', 58, '  reward:', 1)\n",
      "('episode:', 59, '  reward:', 0)\n",
      "('episode:', 60, '  reward:', 0)\n",
      "('episode:', 61, '  reward:', 1)\n",
      "('episode:', 62, '  reward:', 1)\n",
      "('episode:', 63, '  reward:', 1)\n",
      "('episode:', 64, '  reward:', 1)\n",
      "('episode:', 65, '  reward:', 0)\n",
      "('episode:', 66, '  reward:', 0)\n",
      "('episode:', 67, '  reward:', 0)\n",
      "('episode:', 68, '  reward:', 0)\n",
      "('episode:', 69, '  reward:', 0)\n",
      "('episode:', 70, '  reward:', -1)\n",
      "('episode:', 71, '  reward:', -1)\n",
      "('episode:', 72, '  reward:', -1)\n",
      "('episode:', 73, '  reward:', 0)\n",
      "('episode:', 74, '  reward:', 0)\n",
      "('episode:', 75, '  reward:', 0)\n",
      "('episode:', 76, '  reward:', 0)\n",
      "('episode:', 77, '  reward:', 0)\n",
      "('episode:', 78, '  reward:', 0)\n",
      "('episode:', 79, '  reward:', 0)\n",
      "('episode:', 80, '  reward:', 0)\n",
      "('episode:', 81, '  reward:', 0)\n",
      "('episode:', 82, '  reward:', 0)\n",
      "('episode:', 83, '  reward:', 0)\n",
      "('episode:', 84, '  reward:', 0)\n",
      "('episode:', 85, '  reward:', 0)\n",
      "('episode:', 86, '  reward:', 0)\n",
      "('episode:', 87, '  reward:', 0)\n",
      "('episode:', 88, '  reward:', 0)\n",
      "('episode:', 89, '  reward:', 1)\n",
      "('episode:', 90, '  reward:', 1)\n",
      "('episode:', 91, '  reward:', 2)\n",
      "('episode:', 92, '  reward:', 2)\n",
      "('episode:', 93, '  reward:', 2)\n",
      "('episode:', 94, '  reward:', 2)\n",
      "('episode:', 95, '  reward:', 2)\n",
      "('episode:', 96, '  reward:', 1)\n",
      "('episode:', 97, '  reward:', 1)\n",
      "('episode:', 98, '  reward:', 1)\n",
      "('episode:', 99, '  reward:', 2)\n",
      "('episode:', 100, '  reward:', 2)\n",
      "('episode:', 101, '  reward:', 2)\n",
      "('episode:', 102, '  reward:', 2)\n",
      "('episode:', 103, '  reward:', 2)\n",
      "('episode:', 104, '  reward:', 2)\n",
      "('episode:', 105, '  reward:', 2)\n",
      "('episode:', 106, '  reward:', 2)\n",
      "('episode:', 107, '  reward:', 3)\n",
      "('episode:', 108, '  reward:', 3)\n",
      "('episode:', 109, '  reward:', 4)\n",
      "('episode:', 110, '  reward:', 4)\n",
      "('episode:', 111, '  reward:', 8)\n",
      "('episode:', 112, '  reward:', 8)\n",
      "('episode:', 113, '  reward:', 7)\n",
      "('episode:', 114, '  reward:', 7)\n",
      "('episode:', 115, '  reward:', 7)\n",
      "('episode:', 116, '  reward:', 8)\n",
      "('episode:', 117, '  reward:', 9)\n",
      "('episode:', 118, '  reward:', 10)\n",
      "('episode:', 119, '  reward:', 10)\n",
      "('episode:', 120, '  reward:', 13)\n",
      "('episode:', 121, '  reward:', 12)\n",
      "('episode:', 122, '  reward:', 14)\n",
      "('episode:', 123, '  reward:', 17)\n",
      "('episode:', 124, '  reward:', 17)\n",
      "('episode:', 125, '  reward:', 17)\n",
      "('episode:', 126, '  reward:', 17)\n",
      "('episode:', 127, '  reward:', 18)\n",
      "('episode:', 128, '  reward:', 18)\n",
      "('episode:', 129, '  reward:', 17)\n",
      "('episode:', 130, '  reward:', 20)\n",
      "('episode:', 131, '  reward:', 21)\n",
      "('episode:', 132, '  reward:', 23)\n",
      "('episode:', 133, '  reward:', 24)\n",
      "('episode:', 134, '  reward:', 26)\n",
      "('episode:', 135, '  reward:', 25)\n",
      "('episode:', 136, '  reward:', 26)\n",
      "('episode:', 137, '  reward:', 26)\n",
      "('episode:', 138, '  reward:', 28)\n",
      "('episode:', 139, '  reward:', 27)\n",
      "('episode:', 140, '  reward:', 27)\n",
      "('episode:', 141, '  reward:', 27)\n",
      "('episode:', 142, '  reward:', 27)\n",
      "('episode:', 143, '  reward:', 27)\n",
      "('episode:', 144, '  reward:', 26)\n",
      "('episode:', 145, '  reward:', 26)\n",
      "('episode:', 146, '  reward:', 26)\n",
      "('episode:', 147, '  reward:', 25)\n",
      "('episode:', 148, '  reward:', 25)\n",
      "('episode:', 149, '  reward:', 26)\n",
      "('episode:', 150, '  reward:', 26)\n",
      "('episode:', 151, '  reward:', 25)\n",
      "('episode:', 152, '  reward:', 25)\n",
      "('episode:', 153, '  reward:', 28)\n",
      "('episode:', 154, '  reward:', 28)\n",
      "('episode:', 155, '  reward:', 30)\n",
      "('episode:', 156, '  reward:', 29)\n",
      "('episode:', 157, '  reward:', 30)\n",
      "('episode:', 158, '  reward:', 31)\n",
      "('episode:', 159, '  reward:', 32)\n",
      "('episode:', 160, '  reward:', 32)\n",
      "('episode:', 161, '  reward:', 30)\n",
      "('episode:', 162, '  reward:', 30)\n",
      "('episode:', 163, '  reward:', 30)\n",
      "('episode:', 164, '  reward:', 30)\n",
      "('episode:', 165, '  reward:', 31)\n",
      "('episode:', 166, '  reward:', 31)\n",
      "('episode:', 167, '  reward:', 32)\n",
      "('episode:', 168, '  reward:', 34)\n",
      "('episode:', 169, '  reward:', 33)\n",
      "('episode:', 170, '  reward:', 33)\n",
      "('episode:', 171, '  reward:', 33)\n",
      "('episode:', 172, '  reward:', 32)\n",
      "('episode:', 173, '  reward:', 31)\n",
      "('episode:', 174, '  reward:', 31)\n",
      "('episode:', 175, '  reward:', 32)\n",
      "('episode:', 176, '  reward:', 31)\n",
      "('episode:', 177, '  reward:', 34)\n",
      "('episode:', 178, '  reward:', 37)\n",
      "('episode:', 179, '  reward:', 37)\n",
      "('episode:', 180, '  reward:', 36)\n",
      "('episode:', 181, '  reward:', 37)\n",
      "('episode:', 182, '  reward:', 37)\n",
      "('episode:', 183, '  reward:', 36)\n",
      "('episode:', 184, '  reward:', 37)\n",
      "('episode:', 185, '  reward:', 38)\n",
      "('episode:', 186, '  reward:', 37)\n",
      "('episode:', 187, '  reward:', 37)\n",
      "('episode:', 188, '  reward:', 39)\n",
      "('episode:', 189, '  reward:', 39)\n",
      "('episode:', 190, '  reward:', 38)\n",
      "('episode:', 191, '  reward:', 40)\n",
      "('episode:', 192, '  reward:', 42)\n",
      "('episode:', 193, '  reward:', 42)\n",
      "('episode:', 194, '  reward:', 42)\n",
      "('episode:', 195, '  reward:', 41)\n",
      "('episode:', 196, '  reward:', 40)\n",
      "('episode:', 197, '  reward:', 40)\n",
      "('episode:', 198, '  reward:', 38)\n",
      "('episode:', 199, '  reward:', 41)\n",
      "('episode:', 200, '  reward:', 41)\n",
      "('episode:', 201, '  reward:', 43)\n",
      "('episode:', 202, '  reward:', 42)\n",
      "('episode:', 203, '  reward:', 41)\n",
      "('episode:', 204, '  reward:', 41)\n",
      "('episode:', 205, '  reward:', 40)\n",
      "('episode:', 206, '  reward:', 39)\n",
      "('episode:', 207, '  reward:', 39)\n",
      "('episode:', 208, '  reward:', 39)\n",
      "('episode:', 209, '  reward:', 38)\n",
      "('episode:', 210, '  reward:', 40)\n",
      "('episode:', 211, '  reward:', 39)\n",
      "('episode:', 212, '  reward:', 38)\n",
      "('episode:', 213, '  reward:', 37)\n",
      "('episode:', 214, '  reward:', 37)\n",
      "('episode:', 215, '  reward:', 39)\n",
      "('episode:', 216, '  reward:', 38)\n",
      "('episode:', 217, '  reward:', 37)\n",
      "('episode:', 218, '  reward:', 37)\n",
      "('episode:', 219, '  reward:', 40)\n",
      "('episode:', 220, '  reward:', 39)\n",
      "('episode:', 221, '  reward:', 40)\n",
      "('episode:', 222, '  reward:', 40)\n",
      "('episode:', 223, '  reward:', 40)\n",
      "('episode:', 224, '  reward:', 40)\n",
      "('episode:', 225, '  reward:', 47)\n",
      "('episode:', 226, '  reward:', 49)\n",
      "('episode:', 227, '  reward:', 48)\n",
      "('episode:', 228, '  reward:', 47)\n",
      "('episode:', 229, '  reward:', 47)\n",
      "('episode:', 230, '  reward:', 49)\n",
      "('episode:', 231, '  reward:', 49)\n",
      "('episode:', 232, '  reward:', 50)\n",
      "('episode:', 233, '  reward:', 54)\n",
      "('episode:', 234, '  reward:', 54)\n",
      "('episode:', 235, '  reward:', 54)\n",
      "('episode:', 236, '  reward:', 52)\n",
      "('episode:', 237, '  reward:', 53)\n",
      "('episode:', 238, '  reward:', 52)\n",
      "('episode:', 239, '  reward:', 52)\n",
      "('episode:', 240, '  reward:', 54)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('episode:', 241, '  reward:', 53)\n",
      "('episode:', 242, '  reward:', 53)\n",
      "('episode:', 243, '  reward:', 54)\n",
      "('episode:', 244, '  reward:', 53)\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "actor = Actor(sess, n_features=N_F, n_actions=N_A, lr=LR_A)\n",
    "# critic = Critic(sess, n_features=N_F, lr=LR_C)\n",
    "critic = Critic(sess, n_features=N_F, lr=LR_C)     # we need a good teacher, so the teacher should learn faster than the actor\n",
    "MEMORY_SIZE = 10000\n",
    "BATCH_SIZE = 1000\n",
    "replay = ExpReplay(MEMORY_SIZE, N_F, N_A, BATCH_SIZE)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if OUTPUT_GRAPH:\n",
    "    tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "for i_episode in range(MAX_EPISODE):\n",
    "    s = env.reset()\n",
    "    t = 0\n",
    "    track_r = []\n",
    "    while True:\n",
    "        if RENDER: env.render()\n",
    "\n",
    "        a = actor.choose_action(s)\n",
    "        s_, r, done, info = env.step(a)\n",
    "\n",
    "        if done: r = -20\n",
    "\n",
    "        track_r.append(r)\n",
    "\n",
    "        #td_error = critic.learn(s, r, s_)  # gradient = grad[r + gamma * V(s_) - V(s)]\n",
    "        #actor.learn(s, a, td_error)     # true_gradient = grad[logPi(s,a) * td_error]\n",
    "        replay.fifo(s, a, r, s_)\n",
    "        s = s_\n",
    "        t += 1\n",
    "\n",
    "        if done or t >= MAX_EP_STEPS:\n",
    "            ep_rs_sum = sum(track_r)\n",
    "\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.95 + ep_rs_sum * 0.05\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = False  # rendering\n",
    "            print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "            break\n",
    "    states, actions, rewards, next_states = replay.sampling()\n",
    "    mx_actions = np.zeros((actions.shape[0], N_A))\n",
    "    for i in range(0, states.shape[0]):\n",
    "        mx_actions[i, int(actions[i, 0])] = 1\n",
    "    td_errors = critic.get_error(states, rewards, next_states)\n",
    "    actor.learn(states, mx_actions, td_errors)\n",
    "    critic.learn(states, rewards, next_states)\n",
    "    actor.update_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
